<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="DEEP LEARNINGexample解压缩 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929">
<meta property="og:type" content="article">
<meta property="og:title" content="Mivenis">
<meta property="og:url" content="http://example.com/2023/07/11/DEEP%20LEARNING/index.html">
<meta property="og:site_name" content="Mivenis">
<meta property="og:description" content="DEEP LEARNINGexample解压缩 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728172325339.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728172419376.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326220808361.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326220913316.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326220928572.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326221010024.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326221541895.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326221623065.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326221716514.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326221830923.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230328164230258.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230328164357300.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230403115715579.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728221836729.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728221907505.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728221935153.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728221959072.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220729214539093.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220729164448544.png">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-c8f455f645487e978884ae722b3c291b_720w.webp">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230328214542041.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728220138533.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728220227324.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728220951394.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728221057614.png">
<meta property="og:image" content="https://zh-v2.d2l.ai/_images/output_mlp_76f463_66_0.svg">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728221113784.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728221131929.png">
<meta property="og:image" content="https://zh-v2.d2l.ai/_images/output_mlp_76f463_81_0.svg">
<meta property="og:image" content="https://zh-v2.d2l.ai/_images/output_mlp_76f463_96_0.svg">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220729155156417.png">
<meta property="og:image" content="https://zh-v2.d2l.ai/_images/resnet-block.svg">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230406111024426.png">
<meta property="og:image" content="https://zh-v2.d2l.ai/_images/output_language-models-and-dataset_789d14_21_0.svg">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230406125942975.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230406130313361.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230406214248603.png">
<meta property="og:image" content="https://zh-v2.d2l.ai/_images/rnn-train.svg">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230406213435134.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230406213710174.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230406220033397.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230407093848138.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230407094236241.png">
<meta property="og:image" content="https://zh-v2.d2l.ai/_images/deep-rnn.svg">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230407103018081.png">
<meta property="og:image" content="https://zh-v2.d2l.ai/_images/birnn.svg">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230407103739129.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220729234538313.png">
<meta property="og:image" content="https://zh-v2.d2l.ai/_images/seq2seq-details.svg">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220729235429432.png">
<meta property="og:image" content="https://zh-v2.d2l.ai/_images/qkv.svg">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220730001811526.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230407224226018.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220730003304202.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230407225410869.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230407225355675.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220730003748191.png">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-d5c994ac883ec5bf58580a6664714c7c_720w.webp">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-5589e776fd8510eab7a3d87de01580d4_720w.jpg">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220730010249997.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220718204800931.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220718205035765.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220718205103166.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220721104620656.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220721104712686.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220718212844039.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220721104827813.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220721104906610.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220719123252265.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220719114855235.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220719123336259.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220719123402334.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220719123649115.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220721215435960.png">
<meta property="og:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220723115458376.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/294c69bf8c69429b904e85b288dbc86c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTU3Nzg2NA==,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2023-07-11T08:39:48.864Z">
<meta property="article:modified_time" content="2023-06-20T11:59:59.792Z">
<meta property="article:author" content="Geng">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728172325339.png">

<link rel="canonical" href="http://example.com/2023/07/11/DEEP%20LEARNING/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title> | Mivenis</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Mivenis</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/11/DEEP%20LEARNING/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Geng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mivenis">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-11 16:39:48" itemprop="dateCreated datePublished" datetime="2023-07-11T16:39:48+08:00">2023-07-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-20 19:59:59" itemprop="dateModified" datetime="2023-06-20T19:59:59+08:00">2023-06-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="DEEP-LEARNING"><a href="#DEEP-LEARNING" class="headerlink" title="DEEP LEARNING"></a>DEEP LEARNING</h1><h2 id="example"><a href="#example" class="headerlink" title="example"></a>example</h2><p>解压缩</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)<span class="comment">##忽略警告</span></span><br><span class="line"></span><br><span class="line">fname = os.path.join(DATA_ROOT, <span class="string">&quot;aclImdb_v1.tar.gz&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(os.path.join(DATA_ROOT, <span class="string">&quot;aclImdb&quot;</span>)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;从压缩包解压...&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> tarfile.<span class="built_in">open</span>(fname, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.extractall(DATA_ROOT)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">####或者import tarfile</span></span><br><span class="line">tar = tarfile.<span class="built_in">open</span>(<span class="string">&quot;sample.tar.gz&quot;</span>)</span><br><span class="line">tar.extractall()</span><br><span class="line">tar.close()</span><br><span class="line"><span class="comment">####压缩</span></span><br><span class="line">cwd = os.getcwd()</span><br><span class="line">    tar = tarfile.<span class="built_in">open</span>(<span class="string">&#x27;test.tar&#x27;</span>,<span class="string">&#x27;w:gz&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> root ,<span class="built_in">dir</span>,files <span class="keyword">in</span> os.walk(cwd):</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            fullpath = os.path.join(root,file)</span><br><span class="line">            tar.add(fullpath)</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">data_iter = data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br><span class="line">net.to(device)</span><br><span class="line"></span><br><span class="line">loss = nn.MSELoss</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X) ,y)</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">## another</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_samples</span>):</span><br><span class="line">        self.num_samples = num_samples</span><br><span class="line">        self.x = torch.randn(num_samples, <span class="number">10</span>)</span><br><span class="line">        self.y = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (num_samples,))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.num_samples</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x[idx], self.y[idx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = nn.functional.relu(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, dataloader, optimizer, criterion</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0.0</span></span><br><span class="line">    total_correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> dataloader:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(x)</span><br><span class="line">        loss = criterion(output, y)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item() * x.size(<span class="number">0</span>)</span><br><span class="line">        total_correct += (output.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="keyword">return</span> total_loss / <span class="built_in">len</span>(dataloader.dataset), total_correct / <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义测试函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">model, dataloader, criterion</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    total_loss = <span class="number">0.0</span></span><br><span class="line">    total_correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> dataloader:</span><br><span class="line">            output = model(x)</span><br><span class="line">            loss = criterion(output, y)</span><br><span class="line">            total_loss += loss.item() * x.size(<span class="number">0</span>)</span><br><span class="line">            total_correct += (output.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="keyword">return</span> total_loss / <span class="built_in">len</span>(dataloader.dataset), total_correct / <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义主函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="comment"># 定义超参数</span></span><br><span class="line">    num_samples = <span class="number">1000</span></span><br><span class="line">    batch_size = <span class="number">32</span></span><br><span class="line">    learning_rate = <span class="number">0.01</span></span><br><span class="line">    num_epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建数据集和dataloader</span></span><br><span class="line">    dataset = MyDataset(num_samples)</span><br><span class="line">    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建模型、优化器和损失函数</span></span><br><span class="line">    model = MyModel()</span><br><span class="line">    optimizer = optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_loss, train_acc = train(model, dataloader, optimizer, criterion)</span><br><span class="line">        test_loss, test_acc = test(model, dataloader, criterion)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>, Train Loss: <span class="subst">&#123;train_loss:<span class="number">.4</span>f&#125;</span>, Train Acc: <span class="subst">&#123;train_acc:.</span></span></span><br></pre></td></tr></table></figure>



<h2 id="data"><a href="#data" class="headerlink" title="data"></a>data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">12</span>)</span><br><span class="line">x.shape</span><br><span class="line">x.numel()</span><br><span class="line"></span><br><span class="line"><span class="comment">#广播</span></span><br><span class="line"><span class="comment">#通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#对生成的数组执行按元素操作。</span></span><br><span class="line"></span><br><span class="line">X[<span class="number">0</span>:<span class="number">2</span>, :] = <span class="number">12</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.float64) <span class="comment">##继承属性</span></span><br><span class="line">x = torch.randn_like(x, dtype=torch.<span class="built_in">float</span>) <span class="comment"># 指定新的数据类型</span></span><br><span class="line">x=torch.arange(begin,end,step)</span><br><span class="line">x=torch.linspace(begin,end,nums)<span class="comment">##f分成多少份</span></span><br><span class="line">normal(mean,std)/uniform(<span class="keyword">from</span>,to)正态分布/均匀分布</span><br><span class="line">randperm(m)随机排列</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">before = <span class="built_in">id</span>(Y)</span><br><span class="line">Y = Y + X</span><br><span class="line"><span class="built_in">id</span>(Y) == before</span><br><span class="line">false</span><br><span class="line">X[:] = X + Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># adds x to y</span></span><br><span class="line">y.add_(x)</span><br><span class="line">torch.add(x, y, out=result)<span class="comment">##指定到 </span></span><br><span class="line">PyTorch操作inplace版本都有后缀`_`, 例如`x.copy_(y), x.t_()`**</span><br><span class="line">y = x[<span class="number">0</span>, :] <span class="comment">#源tensor也被改了</span></span><br><span class="line">y += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>, :]) <span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#转置</span></span><br><span class="line">A.T</span><br><span class="line"></span><br><span class="line"><span class="comment">#改变形状：</span></span><br><span class="line">torch.reshape(<span class="built_in">input</span>, shape)，</span><br><span class="line"></span><br><span class="line">permute方法是基于Tensor对象的，作用是置换Tensor的某几个维度。</span><br><span class="line"></span><br><span class="line">基于torch库或Tensor对象，作用是置换Tensor的某两个维度</span><br><span class="line"></span><br><span class="line">返回原张量的转置，其对应的shape逆序。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">torch.view</span><br><span class="line">Tensor对象必须是contiguous的，故常与contiguous方法连用：torch.Tensor.contiguous().view(size)，效果等价于torch.Tensor.reshape(shape)</span><br><span class="line"></span><br><span class="line">view()`返回的新`Tensor`与源`Tensor`虽然可能有不同的`size`，但是是共享`data`的，也即更改其中的一个，另外一个也会跟着改变。</span><br><span class="line">y = x.view(<span class="number">15</span>)</span><br><span class="line">z = x.view(-<span class="number">1</span>, <span class="number">5</span>)  <span class="comment"># -1所指的维度可以根据其他维度的值推出来</span></span><br><span class="line"><span class="built_in">print</span>(x.size(), y.size(), z.size())</span><br><span class="line"><span class="comment">#x,y,z共享data，一起改变</span></span><br><span class="line">x_cp = x.clone().view(<span class="number">15</span>)</span><br><span class="line">x -= <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x_cp)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">另外一个常用的函数就是item(), 它可以将一个标量Tensor转换成一个Python number： </span><br><span class="line"></span><br><span class="line"><span class="comment">#原地更改数据</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">id_before = <span class="built_in">id</span>(y)</span><br><span class="line">y[:] = y + x</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(y) == id_before) <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">id_before = <span class="built_in">id</span>(y)</span><br><span class="line">torch.add(x, y, out=y) <span class="comment"># y += x, y.add_(x)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(y) == id_before) <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">A = X.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line"></span><br><span class="line"><span class="comment">#python 标量</span></span><br><span class="line">a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line">a, a.item(), <span class="built_in">float</span>(a), <span class="built_in">int</span>(a)</span><br><span class="line"></span><br><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">b = a.numpy()</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line"><span class="comment">##torch.tensor()将NumPy数组转换成Tensor，需要注意的是该方法总是会进行数据拷贝，返回的Tensor和原来的数据不再共享内存。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="os-x2F-open"><a href="#os-x2F-open" class="headerlink" title="os&#x2F;open"></a>os&#x2F;open</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">os.getcwd()<span class="comment">#返回当前工作目录</span></span><br><span class="line">os.chdir(path)<span class="comment">#改变当前工作目录到指定的路径</span></span><br><span class="line">os.listdir(path)<span class="comment">#返回指定的文件夹包含的文件或文件夹的名字的列表,返回类型为list（str)</span></span><br><span class="line"></span><br><span class="line">os.makedirs(path, exist_ok=<span class="literal">True</span>)<span class="comment">#目录已经存在也不会抛出异常；可以直接创建多层</span></span><br><span class="line">os.mkdir(path[, mode])<span class="comment">#只能一层一层的创建</span></span><br><span class="line"></span><br><span class="line">os.remove(path)<span class="comment">#用于删除指定路径的文件。如果指定的路径是一个目录，将抛出OSError</span></span><br><span class="line">os.removedirs(path)<span class="comment">#用于递归删除目录。像rmdir(), 如果子文件夹成功删除, removedirs()才尝试它们的父文件夹,直到抛出一个error</span></span><br><span class="line">os.rmdir(path)<span class="comment">#用于删除指定路径的目录。仅当这文件夹是空的才可以, 否则, 抛出OSError。</span></span><br><span class="line"></span><br><span class="line">os.rename(src, dst)<span class="comment">#用于命名文件或目录，从 src 到 dst,如果dst是一个存在的目录, 将抛出OSError</span></span><br><span class="line">os.renames(old, new)<span class="comment">#用于递归重命名目录或文件。类似rename()。</span></span><br><span class="line"></span><br><span class="line">os.path.abspath(path)<span class="comment">#返回绝对路径</span></span><br><span class="line">os.path.basename(path)<span class="comment">#返回文件名</span></span><br><span class="line">os.path.exists(path)<span class="comment">#如果路径 path 存在，返回 True；如果路径 path 不存在，返回 False。</span></span><br><span class="line">os.path.dirname(path)<span class="comment">#返回文件路径</span></span><br><span class="line"></span><br><span class="line">os.path.isabs(path)</span><br><span class="line">os.path.isfile(path)</span><br><span class="line">os.path.isdir(path)</span><br><span class="line"></span><br><span class="line">re.sub(<span class="string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, line).strip().lower()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">open</span>(name[, mode[, buffering]])</span><br></pre></td></tr></table></figure>

<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728172325339.png" alt="image-20220728172325339"><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728172419376.png" alt="image-20220728172419376"></p>
<h2 id="function"><a href="#function" class="headerlink" title="function"></a>function</h2><p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326220808361.png" alt="image-20230326220808361"><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326220913316.png" alt="image-20230326220913316"><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326220928572.png" alt="image-20230326220928572"><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326221010024.png" alt="image-20230326221010024"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">torch.mul(a, b)<span class="comment">#对应位相乘，维度必须相等 点乘</span></span><br><span class="line">torch.mm(a, b)<span class="comment">#是矩阵a和b矩阵相乘，矩阵乘法</span></span><br><span class="line">torch.bmm()<span class="comment">#batch</span></span><br><span class="line">torch.matmul()<span class="comment">#利用广播机制进行不同维度的相乘操作</span></span><br><span class="line">torch.dot(x, y)<span class="comment">#点积（dot product）x R^d*1,所以是x.t*y</span></span><br><span class="line">torch.norm(u)<span class="comment"># 二范数</span></span><br><span class="line">torch.<span class="built_in">abs</span>(u).<span class="built_in">sum</span>()<span class="comment">#一范数</span></span><br><span class="line"></span><br><span class="line">layer.__class__.__name__</span><br><span class="line">nn.flatten()<span class="comment">#从从第一维开始，保留batch</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">trans = []</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.append(torchvision.transforms.Resize(size=resize))</span><br><span class="line">    trans.append(torchvision.transforms.ToTensor())</span><br><span class="line">    </span><br><span class="line">    transform = torchvision.transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    </span><br><span class="line">train_set = Data.TensorDataset(*preprocess_imdb(train_data, vocab))</span><br><span class="line">train_iter = Data.DataLoader(train_set, batch_size, shuffle=<span class="literal">True</span>)    </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchtext.vocab <span class="keyword">as</span> vocab</span><br><span class="line"></span><br><span class="line">vocab.pretrained_aliases.keys()</span><br><span class="line">glove = vocab.GloVe(name=<span class="string">&#x27;6B&#x27;</span>, dim=<span class="number">50</span>, cache=cache_dir) <span class="comment"># 与上面等价</span></span><br><span class="line">glove.stoi[<span class="string">&#x27;beautiful&#x27;</span>], glove.itos[<span class="number">3366</span>] <span class="comment"># (3366, &#x27;beautiful&#x27;)</span></span><br><span class="line">k=torchtext.vocab.vocab(collections.Counter([]),min_freq)</span><br><span class="line">k[<span class="string">&#x27;gjh&#x27;</span>]</span><br><span class="line">k.itos()[<span class="number">1</span>]</span><br><span class="line">unk_token = <span class="string">&#x27;&lt;unk&gt;&#x27;</span></span><br><span class="line">default_index = -<span class="number">1</span></span><br><span class="line">v2 = vocab(OrderedDict([(token, <span class="number">1</span>) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]), specials=[unk_token])</span><br><span class="line">v2.set_default_index(default_index)</span><br><span class="line"><span class="built_in">print</span>(v2[<span class="string">&#x27;&lt;unk&gt;&#x27;</span>]) <span class="comment">#prints 0</span></span><br><span class="line"><span class="built_in">print</span>(v2[<span class="string">&#x27;out of vocab&#x27;</span>]) <span class="comment">#prints -1</span></span><br><span class="line"><span class="comment">#make default index same as index of unk_token</span></span><br><span class="line">v2.set_default_index(v2[unk_token])</span><br><span class="line">v2[<span class="string">&#x27;out of vocab&#x27;</span>] <span class="keyword">is</span> v2[unk_token] <span class="comment">#prints True</span></span><br></pre></td></tr></table></figure>



<h2 id="grad"><a href="#grad" class="headerlink" title="grad"></a>grad</h2><p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326221541895.png" alt="image-20230326221541895"><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326221623065.png" alt="image-20230326221623065"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326221716514.png" alt="image-20230326221716514"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230326221830923.png" alt="image-20230326221830923"></p>
<p>深度学习框架通过自动计算导数，即<em>自动微分</em>（automatic differentiation）来加快求导。 实际中，根据设计好的模型，系统会构建一个<em>计算图</em>（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，<em>反向传播</em>（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。</p>
<p>当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的<strong>不是计算微分矩阵</strong>，而是<strong>单独计算批量中每个样本的偏导数之和</strong>。</p>
<p>计算每个样本的微分矩阵和计算每个样本的偏导数之和都是在深度学习中求解梯度的方法，但它们的计算方式和结果的类型略有不同。</p>
<p>loss为一个向量，关于batch size,所以求微分矩阵会比较麻烦</p>
<p>总体来说，<strong>计算每个样本的微分矩阵需要更多的计算量和内存空间，但可以得到更多的导数信息；而计算每个样本的偏导数之和则更加高效，但导数信息更少。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(y.grad_fn)</span><br><span class="line">tensor([[<span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>]], grad_fn=&lt;AddBackward&gt;)</span><br><span class="line">&lt;AddBackward <span class="built_in">object</span> at <span class="number">0x1100477b8</span>&gt;</span><br><span class="line"><span class="built_in">print</span>(x.is_leaf, y.is_leaf) <span class="comment"># True False</span></span><br><span class="line"></span><br><span class="line">z=y.<span class="built_in">sum</span>()</span><br><span class="line">z.backward() <span class="comment">#梯度回传,z为标量</span></span><br><span class="line">x.grad()</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)<span class="comment">#in_place</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。</span></span><br><span class="line"><span class="comment">#只能标量对张量求导 在y.backward()时，如果y是标量，则不需要为backward()传入任何参数；否则，需要传入一个与y同形的Tensor</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">1</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.data) <span class="comment"># 还是一个tensor</span></span><br><span class="line"><span class="built_in">print</span>(x.data.requires_grad) <span class="comment"># 但是已经是独立于计算图之外</span></span><br><span class="line"></span><br><span class="line">y = <span class="number">2</span> * x</span><br><span class="line">x.data *= <span class="number">100</span> <span class="comment"># 只改变了值，不会记录在计算图，所以不会影响梯度传播</span></span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># 更改data的值也会影响tensor的值</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">`<span class="keyword">with</span> torch.no_grad()<span class="comment"># 将不想被追踪的操作代码块包裹起来</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#detach的方法，将variable参数从网络中隔离开，不参与参数更新。</span></span><br><span class="line">y = A(x)</span><br><span class="line">z = B(y.detach())</span><br><span class="line">z.backward()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Probability"><a href="#Probability" class="headerlink" title="Probability"></a>Probability</h2><p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230328164230258.png" alt="image-20230328164230258"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230328164357300.png" alt="image-20230328164357300"></p>
<h2 id="init"><a href="#init" class="headerlink" title="init"></a>init</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.init.uniform_(tensor, a=<span class="number">0.0</span>, b=<span class="number">1.0</span>)</span><br><span class="line">torch.nn.init.normal_(tensor, mean=<span class="number">0.0</span>, std=<span class="number">1.0</span>)</span><br><span class="line">torch.nn.init.constant_(tensor, val)</span><br><span class="line">torch.nn.init.ones_(tensor)</span><br><span class="line">torch.nn.init.xavier_uniform_(tensor, gain=<span class="number">1.0</span>)</span><br><span class="line">torch.nn.init.xavier_normal_(tensor, gain=<span class="number">1.0</span>)</span><br><span class="line">torch.nn.init.kaiming_uniform_(tensor, a=<span class="number">0</span>, mode=<span class="string">&#x27;fan_in&#x27;</span>, nonlinearity=<span class="string">&#x27;leaky_relu&#x27;</span>)</span><br><span class="line">torch.nn.init.kaiming_normal_(tensor, a=<span class="number">0</span>, mode=<span class="string">&#x27;fan_in&#x27;</span>, nonlinearity=<span class="string">&#x27;leaky_relu&#x27;</span>)</span><br><span class="line">torch.nn.init.orthogonal_(tensor, gain=<span class="number">1</span>)</span><br><span class="line">torch.nn.init.sparse_(tensor, sparsity, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>

<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230403115715579.png" alt="image-20230403115715579">xavier_uniform   U(−<em>a</em>,<em>a</em>)<img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728221836729.png" alt="image-20220728221836729"></p>
<p>xavier_normal_    N(0,std2)  <img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728221907505.png" alt="image-20220728221907505"></p>
<p><code>kaiming_uniform_</code>U(−bound,bound)</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728221935153.png" alt="image-20220728221935153"></p>
<p>kaiming_normal_     <img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728221959072.png" alt="image-20220728221959072"></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/5e8e639fa08d">https://www.jianshu.com/p/5e8e639fa08d</a></p>
<h2 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h2><h2 id="network"><a href="#network" class="headerlink" title="network"></a>network</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="comment"># 将训练数据的特征和标签组合</span></span><br><span class="line">dataset = Data.TensorDataset(features, labels)</span><br><span class="line"><span class="comment"># 随机读取小批量</span></span><br><span class="line">data_iter = Data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line"></span><br><span class="line"><span class="comment">#网络构建</span></span><br><span class="line"><span class="comment"># 写法一</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 此处还可以传入其他层</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写法二</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add_module(<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># net.add_module ......</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写法三</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">          <span class="comment"># ......</span></span><br><span class="line">        ]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(param)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br><span class="line"><span class="built_in">print</span>(optimizer)</span><br><span class="line"></span><br><span class="line">optimizer =optim.SGD([</span><br><span class="line">                <span class="comment"># 如果对某个参数不指定学习率，就使用最外层的默认学习率</span></span><br><span class="line">                &#123;<span class="string">&#x27;params&#x27;</span>: net.subnet1.parameters()&#125;, <span class="comment"># lr=0.03</span></span><br><span class="line">                &#123;<span class="string">&#x27;params&#x27;</span>: net.subnet2.parameters(), <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>&#125;</span><br><span class="line">            ], lr=<span class="number">0.03</span>)</span><br><span class="line"></span><br><span class="line">torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd)</span><br><span class="line">nn.Dropout(drop_prob2),   </span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        output = net(X)</span><br><span class="line">        l = loss(output, y.view(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        optimizer.zero_grad() <span class="comment"># 梯度清零，等价于net.zero_grad()</span></span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss: %f&#x27;</span> % (epoch, l.item()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><code>torch.nn</code>仅支持输入一个batch的样本不支持单个样本输入，如果只有单个样本，可使用<code>input.unsqueeze(0)</code>来添加一维。</p>
<h3 id="直接继承module"><a href="#直接继承module" class="headerlink" title="直接继承module"></a>直接继承module</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 调用MLP父类Module的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span></span><br><span class="line">        <span class="comment"># 参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数params</span></span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">784</span>, <span class="number">256</span>) <span class="comment"># 隐藏层</span></span><br><span class="line">        self.act = nn.ReLU()</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        a = self.act(self.hidden(x))</span><br><span class="line">        <span class="keyword">return</span> self.output(a)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(<span class="number">64</span>, <span class="number">32</span>), nn.ReLU())</span><br><span class="line">        self.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="number">16</span>, <span class="number">20</span>), FixedHiddenMLP())<span class="comment">#block</span></span><br><span class="line">chimera(X)</span><br></pre></td></tr></table></figure>

<h3 id="继承module的子类"><a href="#继承module的子类" class="headerlink" title="继承module的子类"></a>继承module的子类</h3><h4 id="sequential"><a href="#sequential" class="headerlink" title="sequential"></a>sequential</h4>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>(MySequential, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(args) == <span class="number">1</span> <span class="keyword">and</span> <span class="built_in">isinstance</span>(args[<span class="number">0</span>], OrderedDict): <span class="comment"># 如果传入的是一个OrderedDict</span></span><br><span class="line">            <span class="keyword">for</span> key, module <span class="keyword">in</span> args[<span class="number">0</span>].items():</span><br><span class="line">                self.add_module(key, module)   </span><br></pre></td></tr></table></figure>
<h4 id="module-list"><a href="#module-list" class="headerlink" title="module list"></a>module list</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = nn.ModuleList([nn.Linear(<span class="number">784</span>, <span class="number">256</span>), nn.ReLU()])</span><br><span class="line">net.append(nn.Linear(<span class="number">256</span>, <span class="number">10</span>)) <span class="comment"># # 类似List的append操作</span></span><br><span class="line"><span class="built_in">print</span>(net[-<span class="number">1</span>])  <span class="comment"># 类似List的索引访问</span></span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"><span class="comment"># net(torch.zeros(1, 784)) # 会报NotImplementedError</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><code>ModuleList</code>仅仅是一个储存各种模块的列表，这些模块之间没有联系也没有顺序（所以不用保证相邻层的输入输出维度匹配），而且没有实现<code>forward</code>功能需要自己实现</p>
<h4 id="module-dict"><a href="#module-dict" class="headerlink" title="module dict"></a>module dict</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">net = nn.ModuleDict(&#123;</span><br><span class="line">    <span class="string">&#x27;linear&#x27;</span>: nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">    <span class="string">&#x27;act&#x27;</span>: nn.ReLU(),</span><br><span class="line">&#125;)</span><br><span class="line">net[<span class="string">&#x27;output&#x27;</span>] = nn.Linear(<span class="number">256</span>, <span class="number">10</span>) <span class="comment"># 添加</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="string">&#x27;linear&#x27;</span>]) <span class="comment"># 访问</span></span><br><span class="line"><span class="built_in">print</span>(net.output)</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"><span class="comment"># net(torch.zeros(1, 784)) # 会报NotImplementedError</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>和<code>ModuleList</code>一样</p>
<h4 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line">OrderedDict([(<span class="string">&#x27;weight&#x27;</span>, tensor([[ <span class="number">0.3016</span>, -<span class="number">0.1901</span>, -<span class="number">0.1991</span>, -<span class="number">0.1220</span>,  <span class="number">0.1121</span>, -<span class="number">0.1424</span>, -<span class="number">0.3060</span>,  <span class="number">0.3400</span>]])), (<span class="string">&#x27;bias&#x27;</span>, tensor([-<span class="number">0.0291</span>]))])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net.named_parameters()))<span class="comment">#访问所有参数</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name, param.size())</span><br><span class="line">(<span class="string">&#x27;0.weight&#x27;</span>, torch.Size([<span class="number">8</span>, <span class="number">4</span>])) (<span class="string">&#x27;0.bias&#x27;</span>, torch.Size([<span class="number">8</span>])) (<span class="string">&#x27;2.weight&#x27;</span>, torch.Size([<span class="number">1</span>, <span class="number">8</span>])) (<span class="string">&#x27;2.bias&#x27;</span>, torch.Size([<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line">OrderedDict([(<span class="string">&#x27;weight&#x27;</span>, tensor([[ <span class="number">0.3231</span>, -<span class="number">0.3373</span>,  <span class="number">0.1639</span>, -<span class="number">0.3125</span>,  <span class="number">0.0527</span>, -<span class="number">0.2957</span>,  <span class="number">0.0192</span>,  <span class="number">0.0039</span>]])), (<span class="string">&#x27;bias&#x27;</span>, tensor([-<span class="number">0.2930</span>]))])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;torch.nn.parameter.Parameter&#x27;</span>&gt;</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([-<span class="number">0.2930</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([-<span class="number">0.2930</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#嵌套块的参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                         nn.Linear(<span class="number">8</span>, <span class="number">4</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># 在这里嵌套</span></span><br><span class="line">        net.add_module(<span class="string">f&#x27;block <span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">rgnet(X)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">#初始化</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;weight&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">        init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        <span class="built_in">print</span>(name, param.data)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="comment">#参数共享</span></span><br><span class="line"><span class="comment">#共享参数通常可以节省内存，并在以下方面具有特定的好处：</span></span><br><span class="line"></span><br><span class="line">对于图像识别中的CNN，共享参数使网络能够在图像中的任何地方而不是仅在某个区域中查找给定的功能。</span><br><span class="line">对于RNN，它在序列的各个时间步之间共享参数，因此可以很好地推广到不同序列长度的示例。</span><br><span class="line">对于自动编码器，编码器和解码器共享参数。 在具有线性激活的单层自动编码器中，共享权重会在权重矩阵的不同隐藏层之间强制正交。</span><br><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="comment"># 检查参数是否相同</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="comment"># 确保它们实际上是同一个对象，而不只是有相同的值</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__(**kwargs)</span><br><span class="line">        self.weight1 = nn.Parameter(torch.rand(<span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">        self.weight2 = torch.rand(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">n = MyModel()</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> n.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name)</span><br><span class="line">weight1</span><br><span class="line"><span class="comment">##将参数定义成`Parameter`，除了直接定义成`Parameter`类外，还可以使用`ParameterList`和`ParameterDict`分别定义参数的列表和字典。</span></span><br><span class="line">self.params = nn.ParameterList([nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)])</span><br><span class="line">self.params.append(nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">self.params = nn.ParameterDict(&#123;</span><br><span class="line">                <span class="string">&#x27;linear1&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)),</span><br><span class="line">                <span class="string">&#x27;linear2&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">self.params.update(&#123;<span class="string">&#x27;linear3&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">2</span>))&#125;) <span class="comment"># 新增</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="save-and-load"><a href="#save-and-load" class="headerlink" title="save and load"></a>save and load</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">torch.save(x, <span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">x2 = torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">x2</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">net.state_dict()</span><br><span class="line"></span><br><span class="line">torch.save(model.state_dict(), PATH) <span class="comment"># 推荐的文件后缀名是pt或pth</span></span><br><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line"></span><br><span class="line"><span class="comment">##multi</span></span><br><span class="line">torch.save(net.module.state_dict(), <span class="string">&quot;.pt&quot;</span>)</span><br><span class="line">new_net.load_state_dict(torch.load(<span class="string">&quot;.pt&quot;</span>)) <span class="comment"># 加载成功</span></span><br></pre></td></tr></table></figure>

<h2 id="fine-tuning"><a href="#fine-tuning" class="headerlink" title="fine tuning"></a>fine tuning</h2><p>微调（fine tuning）。如图9.1所示，微调由以下4步构成。</p>
<ol>
<li>在源数据集（如ImageNet数据集）上预训练一个神经网络模型，即源模型。</li>
<li>创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。<strong>我们还假设源模型的输出层跟源数据集的标签紧密相关</strong>，因此在目标模型中不予采用。</li>
<li>为<strong>目标模型添加一个输出大小为目标数据集类别个数的输出层，</strong>并<strong>随机初始化</strong>该层的模型参数。</li>
<li>在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于<strong>源模型的参数微调</strong>得到的。</li>
</ol>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220729214539093.png" alt="image-20220729214539093"></p>
<p>当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。</p>
<p>一般只需使用<strong>较小的学习率来微调前面层的参数</strong>，而**<code>fc</code>即linear中的随机初始化参数一般需要更大的学习率从头训**练。PyTorch可以方便的对模型的不同部分设置不同的学习参数，我们在下面代码中将<code>fc</code>的学习率设为已经预训练过的部分的10倍。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">output_params = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">id</span>, pretrained_net.fc.parameters()))</span><br><span class="line">feature_params = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: <span class="built_in">id</span>(p) <span class="keyword">not</span> <span class="keyword">in</span> output_params, pretrained_net.parameters())</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">optimizer = optim.SGD([&#123;<span class="string">&#x27;params&#x27;</span>: feature_params&#125;,</span><br><span class="line">                       &#123;<span class="string">&#x27;params&#x27;</span>: pretrained_net.fc.parameters(), <span class="string">&#x27;lr&#x27;</span>: lr * <span class="number">10</span>&#125;],</span><br><span class="line">                       lr=lr, weight_decay=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>



<h2 id="BN-amp-LN"><a href="#BN-amp-LN" class="headerlink" title="BN&amp;LN"></a>BN&amp;LN</h2><h3 id="BN"><a href="#BN" class="headerlink" title="BN"></a>BN</h3><p>由于在训练过程中，中间层的变化幅度不能过于剧烈，而批量规范化将每一层主动居中，并将它们重新调整为给定的平均值和大小</p>
<p>在模型训练过程中，批量规范化利用小批量的均值和标准差，不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。</p>
<p>在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理</p>
<p>对于线性层，</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220729164448544.png" alt="image-20220729164448544"></p>
<p>对于卷积层，我们可以在<strong>卷积层之后和非线性激活函数之前</strong>应用批量规范化。 当卷积有多个输出通道时，我们需要对这些通道的“每个”输出执行批量规范化，每个通道都有自己的<strong>拉伸（scale）和偏移（shift）参数</strong>，这两个参数都是标量在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。</p>
<p>将批量归一化层置于全连接层中的仿射变换和激活函数之间。<em>ϕ</em>(BN(<strong>x</strong>))</p>
<p>对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且<strong>每个通道都拥有独立的拉伸和偏移参数，并均为标量</strong>。</p>
<p>使用批量归一化训练时，我们可以将<strong>批量大小设得大一点</strong>，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望<strong>模型对于任意输入都有确定的输出</strong>。因此，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">           nn.BatchNorm2d(<span class="number">6</span>), <span class="comment">#input_channels</span></span><br><span class="line">           nn.Sigmoid(),</span><br><span class="line">           nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.BatchNorm2d(num_features, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span>)<span class="comment">#num_features即C （N,C,H,W）</span></span><br></pre></td></tr></table></figure>

<p>其规范化针对单个神经元进行，利用网络训练时一个 <strong>batch</strong> 的数据来计算该神经元 X 的<strong>均值</strong>和<strong>方差，</strong> 因而称为 Batch Normalization。</p>
<p><img src="https://pic4.zhimg.com/80/v2-c8f455f645487e978884ae722b3c291b_720w.webp" alt="img"></p>
<p>其中，M是batch的大小。</p>
<p>BN 比较适用的场景是：每个 batch 的数据分布比较接近。所以：在进行训练之前，要做好充分的 <strong>shuffle</strong>，否则会因为batch之间的分布差异较大导致模型优化困难。</p>
<h3 id="LN"><a href="#LN" class="headerlink" title="LN"></a>LN</h3><p>与 BN 不同，LN 是一种横向的规范化，如图所示。它综合考虑一层所有维度的输入，计算<strong>该层的平均输入值和输入方差</strong>，然后用同一个规范化操作来转换各个维度的输入。LN 针对<strong>单个训练样本进行</strong>，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小batch场景、动态网络场景和 RNN，特别是<strong>自然语言处理领域</strong>。此外，LN 不需要额外保存 mini-batch 的均值和方差，节省存储空间。</p>
<p><code>torch.nn.LayerNorm</code>(<em>normalized_shape</em>, <em>eps&#x3D;1e-05</em>, <em>elementwise_affine&#x3D;True</em>, <em>device&#x3D;None</em>, <em>dtype&#x3D;None</em>)</p>
<ul>
<li><p>Input: (N, *)(<em>N</em>,∗)</p>
</li>
<li><p>Output: (N, *)(<em>N</em>,∗)</p>
<p>如果input为(3,5),则对<strong>最后两维度</strong>进行标准化<strong>同层网络的所有隐藏单元共享均值和方差</strong></p>
<p>对于RNNs序列x&#x3D;shape(batch_size, seq_len, hidden_size), 则</p>
<p>ln_mean&#x3D;np.mean(x, axis&#x3D;2)， shape&#x3D;(batch_size, seq_len, 1)</p>
<p>RNNs使用 <strong>Layer Normalization</strong> 对不同时间步进行标准化<strong>（横向标准化：每一个时间步都有自己的分布）</strong>，从而可以处理单一样本、变长序列，而且 <strong>训练和测试处理方式一致</strong>。</p>
<p>Layer normalization（层归一化）是一种常用的神经网络正则化技术，它可以使得神经网络在训练过程中更加稳定和快速收敛。相比于batch normalization（批归一化），layer normalization更适用于RNN等序列模型。</p>
<p>Layer normalization的规则是对于每个样本，对其特征进行归一化，即对每个特征在样本维度上进行均值和方差的计算，然后对该特征进行标准化。具体而言，对于一个输入张量 $x$，其在第 $i$ 个特征维度上的归一化公式为：</p>
<p>$$\text{LayerNorm}(x_i) &#x3D; \frac{x_i - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}} \odot \gamma_i + \beta_i$$</p>
<p>其中 $\mu_i$ 和 $\sigma_i$ 分别是 $x_i$ 在样本维度上的均值和标准差，$\epsilon$ 是一个小常数，$\gamma_i$ 和 $\beta_i$ 是可学习的缩放和偏移参数。</p>
<p>在实际应用中，通常会对输入张量 $x$ 的最后一个维度进行归一化，即 $x$ 的形状为 $(batch_size, seq_len, hidden_size)$，则对于每个样本 $x_{i,j}$，其在最后一个维度上的归一化公式为：</p>
<p>$$\text{LayerNorm}(x_{i,j}) &#x3D; \frac{x_{i,j} - \mu_j}{\sqrt{\sigma_j^2 + \epsilon}} \odot \gamma_j + \beta_j$$</p>
</li>
</ul>
<h3 id="异同"><a href="#异同" class="headerlink" title="异同"></a>异同</h3><p><strong>不同点：</strong></p>
<p>BN 顾名思义是对一个batch进行操作。假设我们有 10行 3列 的数据，即我们的batchsize &#x3D; 10，每一行数据有3个特征，假设这3个特征是【身高、体重、年龄】。那么BN 是针对每一列（特征）进行缩放，例如算出【身高】的均值与方差，再对身高这一列的10个数据进行缩放。体重和年龄同理。这是一种“列缩放”。</p>
<p>而LN方向相反，它针对的是每一行进行缩放。即只看一个数据，算出这个数据所有特征的均值与方差再缩放。这是一种“行缩放”。但某些情形下，layer normalization 对所有的特征进行缩放，显得很没道理。如【身高、体重、年龄】三个特征的均值方差并对其进行缩放，事实上会因为特征的量纲不同而产生很大的影响。但是BN则没有这个影响，因为BN是对一列进行缩放，一列的量纲单位都是相同的。</p>
<p>如果我们将一批文本组成一个batch，那么BN的操作方向是，<strong>对每句话的第一个词进行操作。</strong>但语言文本的复杂性是很高的，任何一个词都有可能放在初始位置，且词序可能并不影响我们对句子的理解。而BN是针对每个位置进行缩放，这不符合NLP的规律。</p>
<p>而LN则是针对一句话进行缩放的，且LN一般用在第三维度，如[batchsize, seq_len, dims]中的dims，<strong>一般为词向量的维度，或者是RNN的输出维度等等</strong>，这一维度各个特征的量纲应该相同。因此也不会遇到上面因为特征的量纲不同而导致的缩放问题。</p>
<h2 id="ERROR"><a href="#ERROR" class="headerlink" title="ERROR"></a>ERROR</h2><p><em>训练误差</em>（training error）是指， 模型在训练数据集上计算得到的误差。 <em>泛化误差</em>（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</p>
<p>我们将重点介绍几个倾向于影响模型泛化的因素。</p>
<ol>
<li><p>可调整参数的数量。当可调整参数的数量（有时称为<em>自由度</em>）很大时，模型往往更容易过拟合。</p>
</li>
<li><p>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</p>
</li>
<li><p>训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。</p>
</li>
</ol>
<p>原则上，<strong>在我们确定所有的超参数之前，我们不希望用到测试集</strong>。 如果我们在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险，那就麻烦大了。 如果我们过拟合了训练数据，还可以在测试数据上的评估来判断过拟合。 但是如果我们过拟合了测试数据，我们又该怎么知道呢？</p>
<p>解决此问题的常见做法是将我们的数据分成三份， 除了训练和测试数据集之外，还增加一个<em>验证数据集</em>（validation dataset）， 也叫<em>验证集</em>（validation set）</p>
<p>原始训练数据被分成K个不重叠的子集。 然后执行K次模型训练和验证，每次在K−1个子集上进行训练， 并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。 最后，通过对K次实验的结果取平均来估计训练和验证误差。</p>
<h2 id="Propagation"><a href="#Propagation" class="headerlink" title="Propagation"></a>Propagation</h2><p>在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。</p>
<p>以上简单网络为例：一方面，在前向传播期间计算正则项 取决于模型参数W(1)和 W(2)的当前值。 它们是由优化算法根据最近迭代的反向传播给出的。 另一方面，反向传播期间参数的梯度计算， 取决于由前向传播给出的隐藏变量ℎ的当前值。</p>
<p>因此，在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播<strong>重复利用前向传播中存储的中间值</strong>，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是<strong>训练比单纯的预测需要更多的内存（显存）</strong>的原因之一。 此外，这些中间值的大小与<strong>网络层的数量和批量的大小</strong>大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致<em>内存不足</em>（out of memory）错误。</p>
<h2 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">torch.cuda.get_device_name(<span class="number">0</span>) </span><br><span class="line">torch.cuda.is_available()</span><br><span class="line"><span class="comment">#选择device</span></span><br><span class="line">torch.cuda.device(<span class="number">1</span>)</span><br><span class="line"><span class="comment">#查看有多少个GPU设备</span></span><br><span class="line">torch.cuda.device_count()</span><br><span class="line"></span><br><span class="line">os.environ[“CUDA_VISIBLE_DEVICES”] = “<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>”</span><br><span class="line"><span class="comment">#需要放在文件的最开始</span></span><br><span class="line"></span><br><span class="line">x = x.cuda(<span class="number">0</span>)</span><br><span class="line">x.device</span><br><span class="line"></span><br><span class="line">net = nn.Linear(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">list</span>(net.parameters())[<span class="number">0</span>].device</span><br><span class="line">net.cuda()</span><br><span class="line"><span class="built_in">list</span>(net.parameters())[<span class="number">0</span>].device</span><br><span class="line"></span><br><span class="line"><span class="comment">## multi gpu</span></span><br><span class="line">net = nn.DataParallel(net, device_ids=devices)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">x = x.to(device)</span><br></pre></td></tr></table></figure>

<h2 id="Optim"><a href="#Optim" class="headerlink" title="Optim"></a>Optim</h2><p>梯度接近或变成零可能是由于当前解在局部最优解附近造成的。事实上，另一种可能性是当前解在鞍点（saddle point）附近。</p>
<p>根据泰勒展开公式，我们得到以下的近似：</p>
<p>$$f(x + \epsilon) \approx f(x) + \epsilon f’(x) .$$</p>
<p>这里$f’(x)$是函数$f$在$x$处的梯度。一维函数的梯度是一个标量，也称导数。</p>
<p>接下来，找到一个常数$\eta &gt; 0$，使得$\left|\eta f’(x)\right|$足够小，那么可以将$\epsilon$替换为$-\eta f’(x)$并得到</p>
<p>$$f(x - \eta f’(x)) \approx f(x) -  \eta f’(x)^2.$$</p>
<p>如果导数$f’(x) \neq 0$，那么$\eta f’(x)^2&gt;0$，所以</p>
<p>$$f(x - \eta f’(x)) \lesssim f(x).$$</p>
<p>这意味着，如果通过</p>
<p>$$x \leftarrow x - \eta f’(x)$$</p>
<p>来迭代$x$，函数$f(x)$的值可能会降低。因此在梯度下降中，我们先选取一个初始值$x$和常数$\eta &gt; 0$，$\eta$通常叫作学习率</p>
<p>如果使用梯度下降，每次自变量迭代的计算开销为$\mathcal{O}(n)$，它随着$n$线性增长。因此，当训练数据样本数很大时，梯度下降每次迭代的计算开销很高。</p>
<p>随机梯度下降（stochastic gradient descent，SGD）减少了每次迭代的计算开销。在随机梯度下降的每次迭代中，我们随机均匀采样的一个样本索引$i\in{1,\ldots,n}$，并计算梯度$\nabla f_i(\boldsymbol{x})$来迭代$\boldsymbol{x}$：</p>
<p>$$\boldsymbol{x} \leftarrow \boldsymbol{x} - \eta \nabla f_i(\boldsymbol{x}).$$</p>
<p>当&#x3D;&#x3D;训练数据集的样本较多时，梯度下降每次迭代的计算开销较大&#x3D;&#x3D;，因而随机梯度下降通常更受青睐</p>
<p>随机梯度随机选取一个样本的损失函数梯度代替所有的损失函数梯度和，降低开销，而且是无偏估计</p>
<p>我们还可以在每轮迭代中随机均匀采样多个样本来组成一个小批量，然后使用这个小批量来计算梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> dataset:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br><span class="line">    loss = loss_fn(output, target)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>



<h3 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_22210253/article/details/85229988">https://blog.csdn.net/qq_22210253/article/details/85229988</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()<span class="comment">##reduction == none,返回和target一样的形状，batch，默认mean，还可以sum</span></span><br><span class="line">loss(x_hat,x)<span class="comment">## x_hat shape(batch,classes),x shape(batch)</span></span><br><span class="line"><span class="comment">#CrossEntropyLoss就是把以上Softmax–Log–NLLLoss合并成一步</span></span><br></pre></td></tr></table></figure>

<h3 id="drop-out"><a href="#drop-out" class="headerlink" title="drop_out"></a>drop_out</h3><p>在探究泛化性之前，我们先来定义一下什么是一个“好”的预测模型？ 我们期待“好”的预测模型能在未知的数据上有很好的表现： 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。简单性的另一个角度是平滑性，即函数不应该对其输入的微小变化敏感。</p>
<p>在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值ℎ以<em>暂退概率</em>p由随机变量ℎ′替换，如下所示：</p>
<p> 暂退法在前向传播过程中，计算每一内部层的同时<strong>注入噪声</strong>，这已经成为训练神经网络的常用技术。 这种方法之所以被称为<em>暂退法</em>，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。神经网络过拟合与<strong>每一层都依赖于前一层激活值相关</strong>，称这种情况为“共适应性”。 </p>
<p>根据此模型的设计，其期望值保持不变，</p>
<p>我们只需在每个全连接层之后添加一个<code>Dropout</code>层， 将暂退概率作为唯一的参数传递给它的构造函数。 在训练时，<code>Dropout</code>层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout1),</span><br></pre></td></tr></table></figure>

<h3 id="wegiht-decay"><a href="#wegiht-decay" class="headerlink" title="wegiht decay"></a>wegiht decay</h3><p>使用L2范数的一个原因是它对权重向量的大分量施加了巨大的惩罚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD([</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].weight,<span class="string">&#x27;weight_decay&#x27;</span>: wd&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].bias&#125;], lr=lr)</span><br></pre></td></tr></table></figure>

<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>Adam算法可以看做是RMSProp算法与动量法的结合</p>
<p><code>torch.optim.Adam</code>(<em>params</em>, <em>lr&#x3D;0.001</em>, <em>betas&#x3D;(0.9, 0.999)</em>, <em>eps&#x3D;1e-08</em>, <em>weight_decay&#x3D;0</em>, <em>amsgrad&#x3D;False</em>, ***, <em>foreach&#x3D;None</em>, <em>maximize&#x3D;False</em>, <em>capturable&#x3D;False</em>)</p>
<h3 id="scheduler"><a href="#scheduler" class="headerlink" title="scheduler"></a>scheduler</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scheduler = lr_scheduler.MultiStepLR(trainer, milestones=[<span class="number">15</span>, <span class="number">30</span>], gamma=<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">for</span> epoch:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step() </span><br></pre></td></tr></table></figure>



<h2 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h2><p>线性意味着<strong><em>单调</em>假设</strong>： 任何特征的增大都会导致模型输出的增大（如果对应的权重为正）， 或者导致模型输出的减小（如果对应的权重为负</p>
<p>我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型</p>
<p>最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前L−1层看作<strong>表示</strong>，把最后一层看作<strong>线性预测器</strong>。 这种架构通常称为<em>多层感知机</em>（multilayer perceptron），通常缩写为<em>MLP</em>。</p>
<p>具有全连接层的多层感知机的参数开销可能会高得令人望而却步</p>
<p> 在仿射变换之后对每个隐藏单元应用非线性的<em>激活函数</em>（activation function）</p>
<p>一般来说，有了激活函数，就不可能再将我们的多层感知机<strong>退化成线性模型</strong></p>
<p>虽然一个单隐层网络能学习任何函数， 但并不意味着我们应该尝试使用单隐藏层网络来解决所有问题。 事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。</p>
<h3 id="activate-function"><a href="#activate-function" class="headerlink" title="activate function"></a>activate function</h3><p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230328214542041.png" alt="image-20230328214542041"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728220138533.png" alt="image-20220728220138533"></p>
<p>当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。 注意，当输入值精确等于0时，ReLU函数不可导</p>
<p><em>pReLU</em>:</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728220227324.png" alt="image-20220728220227324"></p>
<p>即使参数是负的，某些信息仍然可以通过</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728220951394.png" alt="image-20220728220951394"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728221057614.png" alt="image-20220728221057614">d sig &#x3D; sig(1-sig)  <img src="https://zh-v2.d2l.ai/_images/output_mlp_76f463_66_0.svg" alt="../_images/output_mlp_76f463_66_0.svg"></p>
<p>tanh</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728221113784.png" alt="image-20220728221113784"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220728221131929.png" alt="image-20220728221131929"></p>
<p><img src="https://zh-v2.d2l.ai/_images/output_mlp_76f463_81_0.svg" alt="../_images/output_mlp_76f463_81_0.svg"><img src="https://zh-v2.d2l.ai/_images/output_mlp_76f463_96_0.svg" alt="../_images/output_mlp_76f463_96_0.svg"></p>
<h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><ol>
<li><em>平移不变性</em>（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</li>
<li><em>局部性</em>（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而<strong>不过度在意图像中相隔较远区域的关系</strong>，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</li>
</ol>
<p>实际上，卷积运算与互相关运算类似。<strong>为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算</strong>。可见，卷积运算和互相关运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出往往并不相同。</p>
<p>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫<strong>特征图（feature map）</strong></p>
<p>影响元素<em>x</em>的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做<em>x</em>的<strong>感受野（receptive field）</strong></p>
<h3 id="conv"><a href="#conv" class="headerlink" title="conv"></a>conv</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>, padding_mode=<span class="string">&#x27;zeros&#x27;</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span>)<span class="comment">#dilaation为空洞卷积，扩大感受野</span></span><br></pre></td></tr></table></figure>

<p>$$(<em>N</em>,<em>C</em>in,<em>H</em>,<em>W</em>) and output (N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})(<em>N</em>,<em>C</em>out,<em>H</em>out,<em>W</em>out) $$</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220729155156417.png" alt="image-20220729155156417"></p>
<ul>
<li><p>多通道</p>
<p>&#x3D;&#x3D;多输入&#x3D;&#x3D;</p>
<p>troch.sum(torch.cat(,dim&#x3D;0),dim&#x3D;0)</p>
<p>多输出</p>
<p>为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为ci×kh×kw的卷积核张量，这样卷积核的形状是co×ci×kh×kw。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果即卷积后求和。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 对K的第0维遍历，每次同输入X做互相关计算。所有结果使用stack函数合并在一起</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K])</span><br><span class="line"><span class="comment">#</span></span><br></pre></td></tr></table></figure>
<p>1*1 conv</p>
<p>因为使用了最小窗口，1×1卷积失去了卷积层的特有能力——在高度和宽度维度上，识别相邻元素间相互作用的能力。 其实1×1卷积的唯一计算发生在通道上。<strong>输出中的每个元素都是从输入图像中同一位置的元素的线性组合</strong></p>
</li>
</ul>
<h3 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h3><p><strong>降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性</strong></p>
<p><em>最大汇聚层</em>（maximum pooling）和<em>平均汇聚层</em>（average pooling）</p>
<p>在处理多通道输入数据时，<strong>池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加</strong>。这意味着池化层的&#x3D;&#x3D;输出通道数与输入通道数相等。&#x3D;&#x3D;</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.AdaptiveAvgPool2d(output_size)</span><br><span class="line"><span class="comment"># target output size of 5x7</span></span><br><span class="line">m = nn.AdaptiveAvgPool2d((<span class="number">5</span>,<span class="number">7</span>))</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">64</span>, <span class="number">8</span>, <span class="number">9</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># target output size of 7x7 (square)</span></span><br><span class="line">m = nn.AdaptiveAvgPool2d(<span class="number">7</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">64</span>, <span class="number">10</span>, <span class="number">9</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># target output size of 10x7</span></span><br><span class="line">m = nn.AdaptiveAvgPool2d((<span class="literal">None</span>, <span class="number">7</span>))</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">64</span>, <span class="number">10</span>, <span class="number">9</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">torch.nn.AdaptiveMaxPool2d(output_size, return_indices=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># target output size of 5x7</span></span><br><span class="line">m = nn.AdaptiveMaxPool2d((<span class="number">5</span>,<span class="number">7</span>))</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">64</span>, <span class="number">8</span>, <span class="number">9</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h3 id="CrossEntropyLoss-1"><a href="#CrossEntropyLoss-1" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_22210253/article/details/85229988">https://blog.csdn.net/qq_22210253/article/details/85229988</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()<span class="comment">##reduction == none,返回和target一样的形状，batch，默认mean，还可以sum</span></span><br><span class="line">loss(x_hat,x)<span class="comment">## x_hat shape(batch,classes),x shape(batch)</span></span><br></pre></td></tr></table></figure>

<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>残差块第一个卷积部分用stride改变宽高，第二个卷积部分正常，如果ic和oc不同，用1x1conv来调整，最后相加后送入激活函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):  <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channels, num_channels,</span></span><br><span class="line"><span class="params">                 use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                                   kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">            self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">            self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br></pre></td></tr></table></figure>

<p>ResNet模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals,</span></span><br><span class="line"><span class="params">                 first_block=<span class="literal">False</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(input_channels, num_channels,</span><br><span class="line">                                use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line">    <span class="keyword">return</span> blk      </span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5,</span><br><span class="line">                    nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                    nn.Flatten(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>宽高经过b1÷4，经过b2，b3,b4,÷2</p>
<p><img src="https://zh-v2.d2l.ai/_images/resnet-block.svg" alt="../_images/resnet-block.svg"></p>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h3 id="Base"><a href="#Base" class="headerlink" title="Base"></a>Base</h3><p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230406111024426.png" alt="image-20230406111024426"></p>
<p>每个文本序列又被拆分成一个词元列表，<strong><em>词元</em>（token）是文本的基本单位</strong>。 最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。</p>
<p>词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。 现在，让我们构建一个字典，通常也叫做<em>词表</em>（vocabulary）， 用来将字符串类型的词元映射到从00开始的数字索引中。 我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为<em>语料</em>（corpus）。 然后根据每个唯一词元的出现频率，为其分配一个数字索引。 很少出现的词元通常被移除，这可以降低复杂性。 另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“<unk>”。 我们可以选择增加一个列表，用于保存那些被保留的词元， 例如：填充词元（“<pad>”）； 序列开始词元（“<bos>”）； 序列结束词元（“<eos>”）。</p>
<p><img src="https://zh-v2.d2l.ai/_images/output_language-models-and-dataset_789d14_21_0.svg" alt="../_images/output_language-models-and-dataset_789d14_21_0.svg"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230406125942975.png" alt="image-20230406125942975"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230406130313361.png" alt="image-20230406130313361"></p>
<p>&#x3D;&#x3D;自回归：用同一变量之前各期的表现情况，来预测该变量本期的表现情况，并假设它们为线性关系&#x3D;&#x3D;</p>
<p>由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。</p>
<p>随机采样：每个样本都是在原始的长序列上任意捕获的子序列。 在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。</p>
<p>顺序分区&#x2F;相邻采样：保证两个相邻的小批量中的子序列在原始序列上也是相邻的。</p>
<p>隐状态</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230406214248603.png" alt="image-20230406214248603"></p>
<p>由于在当前时间步中， 隐状态使用的定义与前一个时间步中使用的定义相同， 因此 <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/rnn.html#equation-rnn-h-with-state">(8.4.5)</a>的<strong>计算是<em>循环的</em></strong>（recurrent）。 于是基于循环计算的隐状态神经网络被命名为 <em>循环神经网络</em>（recurrent neural network）。 在循环神经网络中执行 <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/rnn.html#equation-rnn-h-with-state">(8.4.5)</a>计算的层 称为<em>循环层</em>（recurrent layer）。</p>
<p><img src="https://zh-v2.d2l.ai/_images/rnn-train.svg" alt="../_images/rnn-train.svg"></p>
<p>困惑度 <img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230406213435134.png" alt="image-20230406213435134"></p>
<p>梯度裁剪 <img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230406213710174.png" alt="image-20230406213710174"></p>
<p><code>rnn_layer</code>的输入形状为(时间步数, 批量大小, 输入个数) 隐藏状态h的形状为(层数, 批量大小, 隐藏单元个数)</p>
<p><code>torch.nn.RNN</code>(*<em>args</em>, **<em>kwargs</em>)</p>
<ul>
<li><p><strong>input_size</strong> – The number of expected features in the input x</p>
</li>
<li><p><strong>hidden_size</strong> – The number of features in the hidden state h</p>
</li>
<li><p><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1</p>
</li>
<li><p><strong>nonlinearity</strong> – The non-linearity to use. Can be either <code>&#39;tanh&#39;</code> or <code>&#39;relu&#39;</code>. Default: <code>&#39;tanh&#39;</code></p>
</li>
<li><p><strong>bias</strong> – If <code>False</code>, then the layer does not use bias weights b_ih and b_hh. Default: <code>True</code></p>
</li>
<li><p><strong>dropout</strong> – If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</p>
</li>
<li><p><strong>bidirectional</strong> – If <code>True</code>, becomes a bidirectional RNN. Default: <code>False</code></p>
</li>
<li><p><strong>input</strong>: <strong>$$h_0$$</strong>:Defaults to zeros if not provided. if bidirecitonal:(2*layers,batch,hiddens)</p>
</li>
</ul>
<h3 id="BPTT"><a href="#BPTT" class="headerlink" title="BPTT"></a>BPTT</h3><p><em>通过时间反向传播</em>（backpropagation through time，BPTT） (<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id182">Werbos, 1990</a>)实际上是循环神经网络中反向传播技术的一个特定应用。它要求我们将循环神经网络的计算图<strong>一次展开一个时间步</strong>， 以获得模型变量和参数之间的依赖关系。 然后，基于链式法则，应用反向传播来计算和存储梯度。 由于序列可能相当长，因此<strong>依赖关系也可能相当长</strong>。 </p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230406220033397.png" alt="image-20230406220033397"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230407093848138.png" alt="image-20230407093848138"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230407094236241.png" alt="image-20230407094236241"></p>
<ul>
<li>“通过时间反向传播”<strong>仅仅适用于反向传播在具有隐状态</strong>的序列模型。</li>
<li>截断是计算方便性和数值稳定性的需要。截断包括：<strong>规则截断和随机截断</strong>。</li>
<li>矩阵的高次幂可能导致神经网络特征值的<strong>发散或消失</strong>，将以梯度爆炸或梯度消失的形式表现。</li>
<li>为了计算的效率，“通过时间反向传播”在计算期间会<strong>缓存中间</strong>值。</li>
</ul>
<h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p>&#x3D;&#x3D;1.早期观测值对预测所有未来观测值具有非常重要的意义。我们希望有某些机制能够在一个记忆元里存储重要的早期信息。 如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度， 因为它会影响所有后续的观测值。&#x3D;&#x3D;更新策略</p>
<p>&#x3D;&#x3D;2.一些词元没有相关的观测值。 例如，在对网页内容进行情感分析时， 可能有一些辅助HTML代码与网页传达的情绪无关。 我们希望有一些机制来<em>跳过</em>隐状态表示中的此类词元。&#x3D;&#x3D;更新策略</p>
<p>&#x3D;&#x3D;3.序列的各个部分之间存在逻辑中断。在这种情况下，最好有一种方法来<em>重置</em>我们的内部状态表示。&#x3D;&#x3D;重置策略</p>
<p> 门控循环单元（gated recurrent unit，GRU）输入是由当前时间步的输入和&#x3D;&#x3D;<strong>前一时间步的隐状态</strong>&#x3D;&#x3D;给出。 两个门的输出是由使用sigmoid激活函数的两个全连接层给出，保证取值在(0,1).</p>
<p>重置门$\boldsymbol{R}_t \in \mathbb{R}^{n \times h}$和更新门$\boldsymbol{Z}<em>t \in \mathbb{R}^{n \times h}$的计算如下：<br>$$<br>\begin{aligned}<br>\boldsymbol{R}<em>t &#x3D; \sigma(\boldsymbol{X}<em>t \boldsymbol{W}</em>{xr} + \boldsymbol{H}</em>{t-1} \boldsymbol{W}</em>{hr} + \boldsymbol{b}<em>r),\<br>\boldsymbol{Z}<em>t &#x3D; \sigma(\boldsymbol{X}<em>t \boldsymbol{W}</em>{xz} + \boldsymbol{H}</em>{t-1} \boldsymbol{W}</em>{hz} + \boldsymbol{b}_z),<br>\end{aligned}<br>$$</p>
<p>其中具体来说，将重置门Rt 与常规隐状态更新机制集成，使用tanh非线性激活函数来确保候选隐状态中的值保持在区间(−1,1)中。时间步$t$的&#x3D;&#x3D;<strong>候选隐藏状态$\tilde{\boldsymbol{H}}_t \in \mathbb{R}^{n \times h}$</strong>&#x3D;&#x3D;的计算为</p>
<p>$$\tilde{\boldsymbol{H}}<em>t &#x3D; \text{tanh}(\boldsymbol{X}<em>t \boldsymbol{W}</em>{xh} + \left(\boldsymbol{R}<em>t \odot \boldsymbol{H}</em>{t-1}\right) \boldsymbol{W}</em>{hh} + \boldsymbol{b}_h),$$</p>
<p>所以当R为0时，之前的隐状态的影响消失；那么我们会更注重于捕捉离我较近的依赖关系</p>
<p>时间步$t$的隐藏状态$\boldsymbol{H}_t \in \mathbb{R}^{n \times h}$的计算使用当前时间步的更新门$\boldsymbol{Z}<em>t$来对上一时间步的隐藏状态$\boldsymbol{H}</em>{t-1}$和当前时间步的候选隐藏状态$\tilde{\boldsymbol{H}}_t$<strong>做组合</strong>：</p>
<p>$$\boldsymbol{H}_t &#x3D; \boldsymbol{Z}<em>t \odot \boldsymbol{H}</em>{t-1}  + (1 - \boldsymbol{Z}_t) \odot \tilde{\boldsymbol{H}}_t.$$</p>
<p>假设更新门在时间步$t’$到$t$（$t’ &lt; t$）之间一直近似1。那么，在时间步$t’$到$t$之间的输入信息几乎没有流入时间步$t$的隐藏状态$\boldsymbol{H}<em>t$。实际上，这可以看作是较早时刻的隐藏状态$\boldsymbol{H}</em>{t’-1}$一直通过时间保存并传递至当前时间步$t$</p>
<ul>
<li>重置门有助于&#x3D;&#x3D;<strong>捕捉时间序列里短期</strong>&#x3D;&#x3D;的依赖关系；</li>
<li>更新门有助于<strong>捕捉时间序列里长期</strong>的依赖关系</li>
</ul>
<p>gru_layer &#x3D; nn.GRU(input_size&#x3D;vocab_size, hidden_size&#x3D;num_hiddens)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rnn_layer, vocab_size, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.num_hiddens = self.rnn.hidden_size</span><br><span class="line">        <span class="comment"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.rnn.bidirectional:</span><br><span class="line">            self.num_directions = <span class="number">1</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.num_directions = <span class="number">2</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens * <span class="number">2</span>, self.vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, state</span>):</span><br><span class="line">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class="line">        X = X.to(torch.float32)</span><br><span class="line">        Y, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span></span><br><span class="line">        <span class="comment"># 它的输出形状是(时间步数*批量大小,词表大小)。</span></span><br><span class="line">        output = self.linear(Y.reshape((-<span class="number">1</span>, Y.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, device, batch_size=<span class="number">1</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.rnn, nn.LSTM):</span><br><span class="line">            <span class="comment"># nn.GRU以张量作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span>  torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class="line">                                 batch_size, self.num_hiddens),</span><br><span class="line">                                device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># nn.LSTM以元组作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span> (torch.zeros((</span><br><span class="line">                self.num_directions * self.rnn.num_layers,</span><br><span class="line">                batch_size, self.num_hiddens), device=device),</span><br><span class="line">                    torch.zeros((</span><br><span class="line">                        self.num_directions * self.rnn.num_layers,</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>LSTM 中引入了3个门，即<strong>输入门（input gate）、遗忘门（forget gate）和输出门（output gate）</strong>，以及<strong>与隐藏状态形状相同的记忆细胞</strong>（某些文献把记忆细胞当成一种特殊的隐藏状态），从而记录额外的信息。为了控制记忆元，我们需要许多门。 其中一个门用来从&#x3D;&#x3D;<strong>单元中输出条目</strong>&#x3D;&#x3D;，我们将其称为<em>输出门</em>（output gate）。 另外一个门用来&#x3D;&#x3D;决定何时将数据读入单元&#x3D;&#x3D;，我们将其称为<em>输入门</em>（input gate）。 我们还需要一种机制来&#x3D;&#x3D;重置单元的内容&#x3D;&#x3D;，由<em>遗忘门</em>（forget gate）来管理， 这种设计的动机与门控循环单元相同， 能够&#x3D;&#x3D;通过专用机制决定什么时候记忆或忽略隐状态中的输入&#x3D;&#x3D;。 </p>
<p>长短期记忆的门的输入均为当前时间步输$X_t$与上一时间步隐藏状态$$H_{t-1}$$，输出由激活函数为sigmoid函数的全连接层计算得到。如此一来，这3个门元素的域均为[0,1]。</p>
<p>时间步$t$的输入门$\boldsymbol{I}_t \in \mathbb{R}^{n \times h}$、遗忘门$\boldsymbol{F}_t \in \mathbb{R}^{n \times h}$和输出门$\boldsymbol{O}_t \in \mathbb{R}^{n \times h}$分别计算如下：</p>
<p>$$<br>\begin{aligned}<br>\boldsymbol{I}<em>t &amp;&#x3D; \sigma(\boldsymbol{X}<em>t \boldsymbol{W}</em>{xi} + \boldsymbol{H}</em>{t-1} \boldsymbol{W}_{hi} + \boldsymbol{b}<em>i),\<br>\boldsymbol{F}<em>t &amp;&#x3D; \sigma(\boldsymbol{X}<em>t \boldsymbol{W}</em>{xf} + \boldsymbol{H}</em>{t-1} \boldsymbol{W}</em>{hf} + \boldsymbol{b}<em>f),\<br>\boldsymbol{O}<em>t &amp;&#x3D; \sigma(\boldsymbol{X}<em>t \boldsymbol{W}</em>{xo} + \boldsymbol{H}</em>{t-1} \boldsymbol{W}</em>{ho} + \boldsymbol{b}_o),<br>\end{aligned}<br>$$</p>
<p>其中的$\boldsymbol{W}<em>{xi}, \boldsymbol{W}</em>{xf}, \boldsymbol{W}<em>{xo} \in \mathbb{R}^{d \times h}$和$\boldsymbol{W}</em>{hi}, \boldsymbol{W}<em>{hf}, \boldsymbol{W}</em>{ho} \in \mathbb{R}^{h \times h}$是权重参数，$\boldsymbol{b}_i, \boldsymbol{b}_f, \boldsymbol{b}_o \in \mathbb{R}^{1 \times h}$是偏差参数。</p>
<p>候选记忆元</p>
<p>时间步$t$的**候选记忆细胞$\tilde{\boldsymbol{C}}_t \in \mathbb{R}^{n \times h}$**的计算为</p>
<p>$$<br>\tilde{\boldsymbol{C}}<em>t &#x3D; \text{tanh}(\boldsymbol{X}<em>t \boldsymbol{W}</em>{xc} + \boldsymbol{H}</em>{t-1} \boldsymbol{W}_{hc} + \boldsymbol{b}_c),<br>$$</p>
<p>其中$\boldsymbol{W}<em>{xc} \in \mathbb{R}^{d \times h}$和$\boldsymbol{W}</em>{hc} \in \mathbb{R}^{h \times h}$是权重参数，$\boldsymbol{b}_c \in \mathbb{R}^{1 \times h}$是偏差参数。</p>
<p>取值范围为[-1,1] &#x3D;&#x3D;与候选隐状态不同，它是有自己的参数的&#x3D;&#x3D;。</p>
<p>记忆元：</p>
<p>当前时间步记忆细胞$\boldsymbol{C}_t \in \mathbb{R}^{n \times h}$的计算组合了&#x3D;&#x3D;上一时间步记忆细胞和当前时间步候选记忆细胞的信息&#x3D;&#x3D;，并通过遗忘门和输入门来控制信息的流动：</p>
<p>$$\boldsymbol{C}_t &#x3D; \boldsymbol{F}<em>t \odot \boldsymbol{C}</em>{t-1} + \boldsymbol{I}_t \odot \tilde{\boldsymbol{C}}_t.$$</p>
<p>&#x3D;&#x3D;输入门决定采取多少备选记忆元的信息，遗忘门决定采取多少过去的记忆元信息&#x3D;&#x3D;</p>
<p>输出门来控制从记忆细胞到隐藏状态$\boldsymbol{H}_t \in \mathbb{R}^{n \times h}$的信息的流动：</p>
<p>$$\boldsymbol{H}_t &#x3D; \boldsymbol{O}_t \odot \text{tanh}(\boldsymbol{C}_t).$$</p>
<p>引入这种设计是为了&#x3D;&#x3D;缓解梯度消失问题， 并更好地捕获序列中的长距离依赖关系&#x3D;&#x3D;。</p>
<p>lstm_layer &#x3D; nn.LSTM(input_size&#x3D;vocab_size, hidden_size&#x3D;num_hiddens)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lstm</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,</span><br><span class="line">     W_hq, b_q] = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * torch.tanh(C)</span><br><span class="line">        Y = (H @ W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, C)</span><br></pre></td></tr></table></figure>



<h3 id="bidirectional"><a href="#bidirectional" class="headerlink" title="bidirectional"></a>bidirectional</h3><p>描述了一个具有L个隐藏层的深度循环神经网络， 每个隐状态都连续地传递到&#x3D;&#x3D;<strong>当前层的下一个时间步和下一层的当前时间步</strong>&#x3D;&#x3D;。</p>
<p><img src="https://zh-v2.d2l.ai/_images/deep-rnn.svg" alt="../_images/deep-rnn.svg"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230407103018081.png" alt="image-20230407103018081"></p>
<p>双向循环神经网络在每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入）。</p>
<p><img src="https://zh-v2.d2l.ai/_images/birnn.svg" alt="../_images/birnn.svg"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230407103739129.png" alt="image-20230407103739129"></p>
<p>双向循环神经网络的计算速度非常慢。 其主要原因是网络的前向传播需要在双向层中进行前向和后向递归， 并且网络的反向传播还依赖于前向传播的结果。 因此，梯度求解将有一个非常长的链。</p>
<p>双向层的使用在实践中非常少，并且仅仅应用于部分场合。 例如，填充缺失的单词、词元注释（例如，用于命名实体识别） 以及作为序列处理流水线中的一个步骤对序列进行编码</p>
<h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p>包含两个主要组件的架构： 第一个组件是一个<em>编码器</em>（encoder）：&#x3D;&#x3D;它接受一个<strong>长度可变的序列</strong>作为输入&#x3D;&#x3D;， 并将其转换为具有&#x3D;&#x3D;固定形状&#x3D;&#x3D;的编码状态。 第二个组件是<em>解码器</em>（decoder）： 它将固定形状的编码状态映射到长度可变的序列。 这被称为<em>编码器-解码器</em>（encoder-decoder）架构。</p>
<p>在编码器接口中，我们只指定长度可变的序列作为编码器的输入<code>X</code>。在下面的解码器接口中，我们新增一个<code>init_state</code>函数， 用于将编码器的输出（<code>enc_outputs</code>）转换为编码后的状态。 注意，此步骤可能需要额外的输入，例如：输入序列的有效长度，  为了逐个地生成长度可变的词元序列， 解码器&#x3D;&#x3D;在每个时间步&#x3D;&#x3D;都会将输入 （例如：在前一时间步生成的词元）和编码后的状态 映射成当前时间步的输出词元。</p>
<p>由于机器翻译数据集由语言对组成， 因此我们可以分别为源语言和目标语言构建两个词表。 使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。 为了缓解这一问题，这里我们<strong>将出现次数少于2次的低频率词元 视为相同的未知</strong>（“<unk>”）词元。 除此之外，我们还指定了额外的特定词元， 例如在小批量时用于<strong>将序列填充到相同长度的填充词元</strong>（“<pad>”）， 以及序列的<strong>开始词元（“<bos>”）和结束词元（“<eos>”）</strong>。 这些特殊词元在自然语言处理任务中比较常用。</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220729234538313.png" alt="image-20220729234538313"></p>
<p>在训练数据集中，我们可以在每个句子后附上特殊符号“&lt;eos&gt;”（end of sequence）以表示序列的终止。编码器每个时间步的输入依次为英语句子中的单词、标点和特殊符号“&lt;eos&gt;”。图中使用了编码器在最终时间步的隐藏状态作为输入句子的表征或编码信息。解码器在各个时间步中使用输入句子的编码信息和上个时间步的输出以及隐藏状态作为输入。我们希望解码器在各个时间步能正确依次输出翻译后的法语单词、标点和特殊符号”&lt;eos&gt;”。需要注意的是，解码器在最初时间步的输入用到了一个表示序列开始的特殊符号”&lt;bos&gt;”（beginning of sequence）。</p>
<p>为了提高计算效率，我们仍然可以通过<em>截断</em>（truncation）和 <em>填充</em>（padding）方式实现一次只处理一个小批量的文本序列。 假设同一个小批量中的每个序列都应该具有相同的长度<code>num_steps</code>， 那么如果文本序列的词元数目少于<code>num_steps</code>时， 我们将继续在其末尾添加特定的“<pad>”词元， 直到其长度达到<code>num_steps</code>； 反之，我们将截断文本序列时，只取其前<code>num_steps</code> 个词元， 并且丢弃剩余的词元。这样，&#x3D;&#x3D;每个文本序列将具有相同的长度， 以便以相同形状的小批量进行加载。&#x3D;&#x3D;</p>
<p>我们将特定的“<eos>”词元添加到所有序列的末尾， 用于表示序列的结束。</p>
<h3 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h3><p>编码器将长度可变的输入序列转换成 形状固定的上下文变量C， 并且将输入序列的信息在该上下文变量中进行编码。</p>
<p>假设输入序列是$x_1,\ldots,x_T$，例如$x_i$是输入句子中的第$i$个词。在时间步$t$，循环神经网络将输入$x_t$的特征向量$\boldsymbol{x}<em>t$和上个时间步的隐藏状态$\boldsymbol{h}</em>{t-1}$变换为当前时间步的隐藏状态$\boldsymbol{h}_t$。我们可以用函数$f$表达循环神经网络隐藏层的变换：<br>$$<br>\boldsymbol{h}_t &#x3D; f(\boldsymbol{x}<em>t, \boldsymbol{h}</em>{t-1}).<br>$$</p>
<p>接下来，编码器通过自定义函数$q$将<strong>各个时间步的隐藏状态</strong>变换为背景变量</p>
<p>$$<br>\boldsymbol{c} &#x3D;  q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T).<br>$$</p>
<p>例如，当选择$q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T) &#x3D; \boldsymbol{h}_T$时，背景变量是输入序列最终时间步的隐藏状态$\boldsymbol{h}_T$。</p>
<p>以上描述的编码器是一个单向的循环神经网络，<strong>每个时间步的隐藏状态只取决于该时间步及之前的输入子序列</strong>。我们也可以使用双向循环神经网络构造编码器。在这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入），并编码了整个序列的信息。</p>
<h3 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h3><p>编码器输出的背景变量$\boldsymbol{c}$编码了整个输入序列$x_1, \ldots, x_T$的信息。给定训练样本中的输出序列$y_1, y_2, \ldots, y_{T’}$，对每个时间步$t’$（符号与输入序列或编码器的时间步$t$有区别），解码器输出$y_{t’}$的条件概率将基于之前的输出序列$y_1,\ldots,y_{t’-1}$和背景变量$\boldsymbol{c}$，<strong>即$P(y_{t’} \mid y_1, \ldots, y_{t’-1}, \boldsymbol{c})$。</strong></p>
<p>解码器将上一时间步的输出$y_{t^\prime-1}$以及背景变量$\boldsymbol{c}$作为输入，并将它们与上一时间步的隐藏状态$\boldsymbol{s}<em>{t^\prime-1}$变换为当前时间步的隐藏状态$\boldsymbol{s}</em>{t^\prime}$。因此，我们可以用函数$g$表达解码器隐藏层的变换：</p>
<p>$$<br>\boldsymbol{s}<em>{t^\prime} &#x3D; g(y</em>{t^\prime-1}, \boldsymbol{c}, \boldsymbol{s}_{t^\prime-1}).<br>$$</p>
<p>当实现解码器时， 我们直接使用&#x3D;&#x3D;<strong>编码器最后一个时间步的隐状态</strong>&#x3D;&#x3D;来初始化解码器的隐状态。 这就要求使用&#x3D;&#x3D;环神经网络实现的编码器和解码器&#x3D;&#x3D;具有相同数量的层和隐藏单元。 为了进一步包含经过编码的输入序列的信息， &#x3D;&#x3D;上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）&#x3D;&#x3D;。 为了预测输出词元的概率分布， 在循环神经网络解码器的&#x3D;&#x3D;最后一层使用全连接层来变换隐状态&#x3D;&#x3D;。</p>
<p>有了解码器的隐藏状态后，我们可以使用自定义的输出层和softmax运算来计算$P(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \boldsymbol{c})$，</p>
<p>解码器接口中，我们新增一个<code>init_state</code>函数， 用于将<strong>编码器的输出（<code>enc_outputs</code>）转换为编码后的状态</strong>。</p>
<p>在训练中我们也可以将标签序列（训练集的真实输出序列）在上一个时间步的标签作为解码器在&#x3D;&#x3D;当前时间步的输入&#x3D;&#x3D;。这叫作强制教学（teacher forcing）</p>
<p><img src="https://zh-v2.d2l.ai/_images/seq2seq-details.svg" alt="../_images/seq2seq-details.svg"></p>
<h3 id="loss-1"><a href="#loss-1" class="headerlink" title="loss"></a>loss</h3><p>在每个时间步，解码器预测了&#x3D;&#x3D;输出词元的概率分布。 类似于语言模型，可以使用&#x3D;&#x3D;softmax来获得分布， 并通过计算交叉熵损失函数来进行优化。 回特定的填充词元被添加到序列的末尾， 因此不同长度的序列可以以相同形状的小批量加载。 但是，我们应该将&#x3D;&#x3D;填充词元的预测排除在损失函数的计算之外&#x3D;&#x3D;。</p>
<h3 id="evaluate"><a href="#evaluate" class="headerlink" title="evaluate"></a>evaluate</h3><p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220729235429432.png" alt="image-20220729235429432"></p>
<ul>
<li>BLEU是一种常用的评估方法，它通过测量预测序列和标签序列之间的�元语法的匹配度来评估预测。</li>
</ul>
<h3 id="beam-search"><a href="#beam-search" class="headerlink" title="beam search"></a>beam search</h3><p>&#x3D;&#x3D;如果精度最重要，则显然是穷举搜索。 如果计算成本最重要，则显然是贪心搜索。 而束搜索的实际应用则介于这两个极端之间&#x3D;&#x3D;</p>
<p><em>束搜索</em>（beam search）是贪心搜索的一个改进版本，有一个超参数，名为<em>束宽</em>（beam size）k。 在时间步11，我们选择具有最高条件概率的k个词元。</p>
<h3 id="EncoderDecoder-Strcture"><a href="#EncoderDecoder-Strcture" class="headerlink" title="EncoderDecoder Strcture"></a>EncoderDecoder Strcture</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">        <span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">  <span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqDecoder</span>(d2l.Decoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="comment"># 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 广播context，使其具有与X相同的num_steps</span></span><br><span class="line">        context = state[-<span class="number">1</span>].repeat(X.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        X_and_context = torch.cat((X, context), <span class="number">2</span>)</span><br><span class="line">        output, state = self.rnn(X_and_context, state)</span><br><span class="line">        output = self.dense(output).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># output的形状:(batch_size,num_steps,vocab_size)</span></span><br><span class="line">        <span class="comment"># state[0]的形状:(num_layers,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure>







<h2 id="Attention-is-all-u-need"><a href="#Attention-is-all-u-need" class="headerlink" title="Attention is all u need"></a>Attention is all u need</h2><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>以循环神经网络为例，注意力机制通过&#x3D;&#x3D;对编码器所有时间步的隐藏状态做加权平均&#x3D;&#x3D;来得到背景变量。解码器在每一时间步调整这些权重，即注意力权重，从而能够&#x3D;&#x3D;在不同时间步分别关注输入序列中的不同部分&#x3D;&#x3D;并编码进相应时间步的背景变量。</p>
<p>在注意力机制中，解码器的每一时间步将&#x3D;&#x3D;<strong>使用可变的背景变量</strong>&#x3D;&#x3D;。记$\boldsymbol{c}_{t’}$是解码器在时间步$t’$的背景变量，那么解码器在该时间步的隐藏状态可以改写为</p>
<p>$$\boldsymbol{s}<em>{t’} &#x3D; g(\boldsymbol{y}</em>{t’-1}, \boldsymbol{c}<em>{t’}, \boldsymbol{s}</em>{t’-1}).$$</p>
<p>关键是如何&#x3D;&#x3D;计算背景变量$\boldsymbol{c}<em>{t’}$和如何利用它来更新隐藏状态$\boldsymbol{s}</em>{t’}$&#x3D;&#x3D;</p>
<p>自主性提示为带有主观意愿性质的，非自主性提示则为基于环境或者本身自有的</p>
<p>“是否包含自主性提示”将&#x3D;&#x3D;注意力机制与全连接层或汇聚层区别开来&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;自主性提示称为<em>查询</em>（query）&#x3D;&#x3D;。给定任何查询，注意力机制通过<em>注意力汇聚</em>（attention pooling） 将选择引导至<em>感官输入</em>（sensory inputs，例如中间特征表示）。 在注意力机制中，这些&#x3D;&#x3D;感官输入被称为<em>值</em>（value）&#x3D;&#x3D;。 更通俗的解释，每个值都与一个<em>键</em>（key）配对， 这可以想象为&#x3D;&#x3D;感官输入的非自主提示key&#x3D;&#x3D;。</p>
<p>我们可以设计注意力汇聚， 以便给定的&#x3D;&#x3D;查询（自主性提示）可以与键（非自主性提示）进行匹配&#x3D;&#x3D;， 这将引导得出最匹配的值（感官输入）。</p>
<p>&#x3D;&#x3D;整个逻辑就是q(自主性提示)对k(非自主性提示)进行调整，调整后的结果导出最终的值&#x3D;&#x3D;</p>
<p><img src="https://zh-v2.d2l.ai/_images/qkv.svg" alt="../_images/qkv.svg"></p>
<p>注意力汇聚得到的是加权平均的总和值，&#x3D;&#x3D;其中权重是在给定的查询和不同的键之间计算得出的&#x3D;&#x3D;</p>
<p>广义上，注意力机制的输入包括查询项以及一一对应的键项和值项，其中&#x3D;&#x3D;值项是需要加权平均的一组项&#x3D;&#x3D;。在加权平均中，值项的权重来自查询项以及与该值项对应的键项的计算。</p>
<p>在上面的例子中，&#x3D;&#x3D;查询项为解码器的隐藏状态&#x3D;&#x3D;，键项和值项均为编码器的隐藏状态，因为背景变量是编码器隐状态的加权。</p>
<p>隐藏状态</p>
<p>至于背景变量$c_{t’}$如何计算，函数<em>a</em>根据解码器在前一时间步的&#x3D;&#x3D;隐藏状态&#x3D;&#x3D;和&#x3D;&#x3D;编码器&#x3D;&#x3D;在各个时间步的隐藏状态计算&#x3D;&#x3D;softmax运算的输入&#x3D;&#x3D;。softmax运算输出&#x3D;&#x3D;概率分布&#x3D;&#x3D;并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量。</p>
<p>具体来说，令编码器在时间步$t$的隐藏状态为$\boldsymbol{h}_t$，且总时间步数为$T$。那么解码器在时间步$t’$的背景变量为&#x3D;&#x3D;所有编码器隐藏状态的加权平均&#x3D;&#x3D;：</p>
<p>$$<br>\boldsymbol{c}<em>{t’} &#x3D; \sum</em>{t&#x3D;1}^T \alpha_{t’ t} \boldsymbol{h}_t,<br>$$</p>
<p>其中给定$t’$时，权重$\alpha_{t’ t}$在$t&#x3D;1,\ldots,T$的值是一个概率分布。为了得到概率分布，我们可以使用softmax运算:</p>
<p>$$<br>\alpha_{t’ t} &#x3D; \frac{\exp(e_{t’ t})}{ \sum_{k&#x3D;1}^T \exp(e_{t’ k}) },\quad t&#x3D;1,\ldots,T.<br>$$</p>
<p>现在，&#x3D;&#x3D;我们需要定义如何计算上式中softmax运算的输入$e_{t’ t}$&#x3D;&#x3D;。由于$e_{t’ t}$同时&#x3D;&#x3D;取决于解码器的时间步$t’$和编码器的时间步$t$，&#x3D;&#x3D;我们不妨以解码器在时间步$t’-1$的隐藏状态$\boldsymbol{s}_{t’ - 1}$与编码器在时间步$t$的隐藏状态$\boldsymbol{h}<em>t$为输入，并通过函数$a$计算$e</em>{t’ t}$：</p>
<p>$$<br>e_{t’ t} &#x3D; a(\boldsymbol{s}_{t’ - 1}, \boldsymbol{h}_t).<br>$$</p>
<p>假设我们希望根据解码器单个隐藏状态$\boldsymbol{s}<em>{t’ - 1} \in \mathbb{R}^{h}$和编码器所有隐藏状态$\boldsymbol{h}<em>t \in \mathbb{R}^{h}, t &#x3D; 1,\ldots,T$来计算背景向量$\boldsymbol{c}</em>{t’}\in \mathbb{R}^{h}$。<br>我们可以将查询项矩阵$\boldsymbol{Q} \in \mathbb{R}^{1 \times h}$设为$\boldsymbol{s}</em>{t’ - 1}^\top$，并令键项矩阵$\boldsymbol{K} \in \mathbb{R}^{T \times h}$和值项矩阵$\boldsymbol{V} \in \mathbb{R}^{T \times h}$&#x3D;&#x3D;<strong>相同且第$t$行均为$\boldsymbol{h}_t^\top$</strong>&#x3D;&#x3D;。此时，我们只需要通过矢量化计算</p>
<p>$$\text{softmax}(\boldsymbol{Q}\boldsymbol{K}^\top)\boldsymbol{V}$$</p>
<p>即可算出<strong>转置后的背景向量</strong>$\boldsymbol{c}_{t’}^\top$。当查询项矩阵$\boldsymbol{Q}$的行数为$n$时，上式将得到$n$行的输出矩阵。输出矩阵与查询项矩阵在相同行上一一对应。这里函数$a$​有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积$a(\boldsymbol{s}, \boldsymbol{h})&#x3D;\boldsymbol{s}^\top \boldsymbol{h}$​。当查询和键是&#x3D;&#x3D;不同长度的矢量&#x3D;&#x3D;时， 我们可以使用加性注意力作为评分函数：<br>$$<br>a(\boldsymbol{s}, \boldsymbol{h}) &#x3D; \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s} + \boldsymbol{W}_h \boldsymbol{h}),<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AdditiveAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AdditiveAttention, self).__init__(**kwargs)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_v = nn.Linear(num_hiddens, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        queries, keys = self.W_q(queries), self.W_k(keys)</span><br><span class="line">        <span class="comment"># 在维度扩展后，</span></span><br><span class="line">        <span class="comment"># queries的形状：(batch_size，查询的个数，1，num_hidden)</span></span><br><span class="line">        <span class="comment"># key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># 使用广播方式进行求和</span></span><br><span class="line">        features = queries.unsqueeze(<span class="number">2</span>) + keys.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        features = torch.tanh(features)</span><br><span class="line">        <span class="comment"># self.w_v仅有一个输出，因此从形状中移除最后那个维度。</span></span><br><span class="line">        <span class="comment"># scores的形状：(batch_size，查询的个数，“键-值”对的个数)</span></span><br><span class="line">        scores = self.w_v(features).squeeze(-<span class="number">1</span>)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure>

<p>其中$\boldsymbol{v}$、$\boldsymbol{W}_s$、$\boldsymbol{W}_h$都是可以学习的模型参数。</p>
<p>&#x3D;&#x3D;使用点积可以得到计算效率更高的评分函数， 但是点积操作要求查询和键具有相同的长度d&#x3D;&#x3D;。 假设查询和键的所有元素都是独立的随机变量， 并且都满足零均值和单位方差， 那么两个向量的点积的均值为0，方差为d。 &#x3D;&#x3D;为确保无论向量长度如何， 点积的方差在不考虑向量长度的情况下仍然是1&#x3D;&#x3D;， 我们将点积除以√d， 则<em>缩放点积注意力</em>（scaled dot-product attention）评分函数为</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220730001811526.png" alt="image-20220730001811526"></p>
<ul>
<li><p>&#x3D;&#x3D;当查询和键是不同长度的矢量时，可以使用可加性注意力评分函数。当它们的长度相同时，使用缩放的“点－积”注意力评分函数的计算效率更高。&#x3D;&#x3D;</p>
</li>
<li><p>首先，初始化解码器的状态，需要下面的输入：</p>
<ol>
<li>编码器在&#x3D;&#x3D;所有时间步的最终层隐状态&#x3D;&#x3D;，将作为注意力的键和值；</li>
<li>&#x3D;&#x3D;上一时间步的编码器全层隐状态&#x3D;&#x3D;，将作为初始化解码器的隐状态；</li>
<li>编码器有效长度（排除在注意力池中填充词元）。</li>
</ol>
<p>在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。 因此，&#x3D;&#x3D;注意力输出和输入嵌入&#x3D;&#x3D;都连结为循环神经网络解码器的输入。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqAttentionDecoder</span>(<span class="title class_ inherited__">AttentionDecoder</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.AdditiveAttention(</span><br><span class="line">            num_hiddens, num_hiddens, num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">            dropout=dropout)<span class="comment">##c和x一起送入rnn</span></span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span><br><span class="line">        <span class="comment"># outputs的形状为(batch_size，num_steps，num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers，batch_size，num_hiddens)</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state, enc_valid_lens)<span class="comment">#返回的是个3元的tuple</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="comment"># enc_outputs的形状为(batch_size,num_steps,num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers,batch_size,num_hiddens)</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_lens = state<span class="comment">##这个state即为init_state的返回值，三个</span></span><br><span class="line">        <span class="comment"># 输出X的形状为(num_steps,batch_size,embed_size)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        outputs, self._attention_weights = [], []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            <span class="comment"># query的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            query = torch.unsqueeze(hidden_state[-<span class="number">1</span>], dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># context的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            context = self.attention(</span><br><span class="line">                query, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">            <span class="comment"># 在特征维度上连结</span></span><br><span class="line">            x = torch.cat((context, torch.unsqueeze(x, dim=<span class="number">1</span>)), dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 将x变形为(1,batch_size,embed_size+num_hiddens)</span></span><br><span class="line">            out, hidden_state = self.rnn(x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">            self._attention_weights.append(self.attention.attention_weights)</span><br><span class="line">        <span class="comment"># 全连接层变换后，outputs的形状为</span></span><br><span class="line">        <span class="comment"># (num_steps,batch_size,vocab_size)</span></span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                          enc_valid_lens]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure>



<h3 id="Multihead"><a href="#Multihead" class="headerlink" title="Multihead"></a>Multihead</h3><p>当给定相同的查询、键和值的集合时， 我们希望模型可以基于相同的注意力机制&#x3D;&#x3D;<strong>学习到不同的行为</strong>&#x3D;&#x3D;， 然后将不同的行为作为知识组合起来， 捕获序列内各种范围的依赖关系 （例如，短距离依赖和长距离依赖关系）。 因此，允许注意力机制组合使用查询、键和值的不同 <em>子空间表示</em>（representation subspaces）可能是有益的</p>
<p>多头注意力的输出需要经过另一个线性转换，基于这种设计，&#x3D;&#x3D;每个头都可能会关注输入的不同部分&#x3D;&#x3D;， 可以表示比简单加权平均值更复杂的函数</p>
<p>为了能够使多个头并行计算,需要进行如下操作,&#x3D;&#x3D;先permute才能reshape&#x3D;&#x3D;</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transpose_qkv</span>(<span class="params">X, num_heads</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;为了多注意力头的并行计算而变换形状&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    <span class="keyword">return</span> X.reshape(-<span class="number">1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transpose_output</span>(<span class="params">X, num_heads</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot;</span></span><br><span class="line">    X = X.reshape(-<span class="number">1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;多头注意力&quot;&quot;&quot;</span></span><br><span class="line">   </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        <span class="comment"># queries，keys，values的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># valid_lens　的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size，)或(batch_size，查询的个数)</span></span><br><span class="line">        <span class="comment"># 经过变换后，输出的queries，keys，values　的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size*num_heads，查询或者“键－值”对的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        queries = transpose_qkv(self.W_q(queries), self.num_heads)</span><br><span class="line">        keys = transpose_qkv(self.W_k(keys), self.num_heads)</span><br><span class="line">        values = transpose_qkv(self.W_v(values), self.num_heads)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 在轴0，将第一项（标量或者矢量）复制num_heads次，</span></span><br><span class="line">            <span class="comment"># 然后如此复制第二项，然后诸如此类。</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(</span><br><span class="line">                valid_lens, repeats=self.num_heads, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output的形状:(batch_size*num_heads，查询的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        output = self.attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output_concat的形状:(batch_size，查询的个数，num_hiddens)</span></span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure>

<p>本质上就是对原有的一个query(q,h),分成heads份来进行学习，每个学习h&#x2F;heads部分</p>
<h3 id="self-attention-and-position-encoding"><a href="#self-attention-and-position-encoding" class="headerlink" title="self-attention and position-encoding"></a>self-attention and position-encoding</h3><p>有了注意力机制之后，我们将词元序列输入注意力池化中， 以便&#x3D;&#x3D;同一组词元同时充当查询、键和值&#x3D;&#x3D;。 具体来说，每个查询都会关注所有的键－值对并生成一个注意力输出。 由于查询、键和值来自同一组输入，因此被称为 <em>自注意力</em>（self-attention）<img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230407224226018.png" alt="image-20230407224226018"></p>
<p>比较自注意力和RNN,CNN</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220730003304202.png" alt="image-20220730003304202"></p>
<p>CNN:卷积核为k，通道数为d，序列长度为n,则计算复杂度为O(nkd^2),顺序操作为O(1),最大路径长度(一个输入连接到另一个输入的长度)为O(n&#x2F;k),</p>
<p>RNN:计算复杂度O(nd^2),顺序操作O(n) ,最大路径长度O(n)</p>
<p>Self attention: 计算复杂度O(n^2d),顺序操作O(1),最大路径长度O(1)</p>
<p>卷积神经网络和自注意力都拥有&#x3D;&#x3D;并行计算&#x3D;&#x3D;的优势， 而且自注意力的最大路径长度最短。 但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢</p>
<p>位置编码：为了并行计算，自注意力放弃了位置信息，为了对不同输入的位置信息进行区分，引入位置编码来注入绝对的或相对的位置信息，可以&#x3D;&#x3D;通过学习也可以人为固定&#x3D;&#x3D;</p>
<p>关于positional embedding ，文章提出两种方法：</p>
<p>1.Learned Positional Embedding ，这个是绝对位置编码，即直接对不同的位置随机初始化一个postion embedding，这个postion embedding作为参数进行训练。</p>
<p>2.Sinusoidal Position Embedding ，相对位置编码，即三角函数编码。</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230407225410869.png" alt="image-20230407225410869"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20230407225355675.png" alt="image-20230407225355675"></p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>自注意力同时具有&#x3D;&#x3D;并行计算和最短的最大路径长度&#x3D;&#x3D;这两个优势.transformer模型完全基于注意力机制，没有任何卷积层或循环神经网络层 </p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/166608727">https://zhuanlan.zhihu.com/p/166608727</a></p>
<p>transformer的编码器和解码器是基于自注意力的模块叠加而成的，源（输入）序列和目标（输出）序列的<em>嵌入</em>（embedding）表示将加上<em>位置编码</em>（positional encoding），再分别输入到编码器和解码器中</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220730003748191.png" alt="image-20220730003748191"></p>
<h3 id="encoder-1"><a href="#encoder-1" class="headerlink" title="encoder"></a>encoder</h3><p>两个子层，第一个子层是<em>多头自注意力</em>（multi-head self-attention）汇聚；第二个子层是<em>基于位置的前馈网络</em>（positionwise feed-forward network）。</p>
<p>在计算注意力时，查询、键和值都来自前一个编码器层的输出</p>
<p>在Transformer中编码器的注意力计算中，查询（Query）和键值（Key-Value）是用于计算注意力权重的重要元素。</p>
<p><strong>在注意力计算过程中，每个编码器层接收来自前一层的输入，其中每个输入向量都被分为三个部分：查询（Q）、键（K）和值（V）。这三个部分通过线性变换的方式从输入向量中获取，这些线性变换是通过学习的参数矩阵完成的。</strong>通常，在不同的自注意力机制实现中，这些线性变换可能是共享的，也就是说，同样的参数矩阵被用于生成每个查询、键和值。</p>
<p>查询（Q）用于计算注意力分数和注意力权重。对于每一个查询，注意力权重表示该查询与其他输入的相关性或重要性。</p>
<p>键（K）和值（V）则提供了额外的信息。键（K）用于衡量查询（Q）与其他输入之间的相关性，而值（V）则包含了编码器层对应输入的信息。</p>
<p>注意力权重是通过查询（Q）和键（K）之间的相似性计算得出的，常用的方法是计算查询和键的点积，并通过缩放因子进行归一化。然后，使用注意力权重对值（V）进行加权求和，以获得最终的注意力表示。</p>
<p>总结起来，查询（Q）用于计算注意力分数，键（K）用于计算注意力关注的对象，值（V）含有对应的信息。注意力权重是基于查询和键之间的相对关系计算得出的，用于加权求和编码器层的值（V）得到最终的注意力表示。</p>
<p>对于序列中任何位置的任何输入X，sublayer(X)与X相同形状，方便进行残差连接，残差后进行层规范化</p>
<p>自注意力权重的形状为（编码器层数，注意力头数，<code>num_steps</code>或查询的数目，<code>num_steps</code>或“键－值”对的数目）。</p>
<p>输入的形状为(batch_size,seq_len,depth),句子需要补长，变为指定长度，需要padding，padding后不应该把注意力放在这些补齐的地方，把这些未知的值加上负无穷，在softmax后相应的权重为0即attentionweight为0</p>
<p><strong>相对于Post-LN，Pre-LN能够提升Transformer的稳定性。然而，Pre-LN在底层的梯度往往大于顶层，导致其性能不及Post-LN</strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-d5c994ac883ec5bf58580a6664714c7c_720w.webp" alt="img"></p>
<p><strong>encoder输入输出</strong></p>
<p>让我们从输入开始，再从头理一遍单个encoder这个过程:</p>
<ul>
<li>输入x</li>
<li>x 做一个层归一化： x1 &#x3D; norm(x)</li>
<li>进入多头self-attention: x2 &#x3D; self_attention(x1)</li>
<li>残差加成：x3 &#x3D; x + x2</li>
<li>再做个层归一化：x4 &#x3D; norm(x3)</li>
<li>经过前馈网络: x5 &#x3D; feed_forward(x4)</li>
<li>残差加成: x6 &#x3D; x3 + x5</li>
<li>输出x6</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;transformer编码器块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span><br><span class="line"><span class="params">                 dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout,</span><br><span class="line">            use_bias)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)<span class="comment">##使用了layernorm和残差连接</span></span><br><span class="line">        self.ffn = PositionWiseFFN(</span><br><span class="line">            ffn_num_input, ffn_num_hiddens, num_hiddens)<span class="comment">##linear relu linear</span></span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, valid_lens</span>):</span><br><span class="line">        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))</span><br><span class="line">        <span class="keyword">return</span> self.addnorm2(Y, self.ffn(Y))</span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFFN</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span></span><br><span class="line"><span class="params">                 **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.dense2(self.relu(self.dense1(X)))</span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AddNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, normalized_shape, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.ln = nn.LayerNorm(normalized_shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, Y</span>):</span><br><span class="line">        <span class="keyword">return</span> self.ln(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure>

<p>由于我们使用的是值范围在−−1和1之间的固定位置编码，因此通过学习得到的输入的嵌入表示的值需要先乘以嵌入维度的平方根进行重新缩放，然后再与位置编码相加,这是因为用的是点积即d个数据相加，会造成方差的扩大</p>
<p>位置编码分为两种，</p>
<p>1.Learned Positional Embedding ，这个是绝对位置编码，即直接对不同的位置随机初始化一个postion embedding，这个postion embedding作为参数进行训练。</p>
<p>2.Sinusoidal Position Embedding ，相对位置编码，即三角函数编码。</p>
<p>由下述公式得到Embedding值:</p>
<p><img src="https://pic1.zhimg.com/80/v2-5589e776fd8510eab7a3d87de01580d4_720w.jpg" alt="img"></p>
<p>对于句子中的每一个字，其位置pos∈<a href="%E5%81%87%E8%AE%BE%E6%AF%8F%E5%8F%A5%E8%AF%9D10%E4%B8%AA%E5%AD%97">0,1,2,…,9</a>, 每个字是N（512）维向量，维度 i （i∈[ 0,1,2,3,4,..N]）带入函数。</p>
<p>对每个positional embedding进行 sin 或者cos激活，可能效果更好，那就再将偶数列上的embedding值用sin()函数激活，奇数列的embedding值用cos()函数激活得到的具体示意图如下:<img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220730010249997.png" alt="image-20220730010249997"></p>
<p>这样使用三角函数设计的好处是&#x3D;&#x3D;位置 i 处的单词的psotional embedding可以被位置 i+k 处单词的psotional embedding线性表示&#x3D;&#x3D;，反应两处单词的其相对位置关系。此外位置i和i+k的psotional embedding内积会随着&#x3D;&#x3D;相对位置的递增而减小&#x3D;&#x3D;，从而表征位置的相对距离。</p>
<p>但是不难发现，&#x3D;&#x3D;由于距离的对称性，inusoidal Position Encoding虽然能够反映相对位置的距离关系，但是无法区分i和i+j的方向&#x3D;&#x3D;。</p>
<p><strong>从参数维度上</strong>，使用三角函数Position Encoding不会引入额外参数，Learned Positional Embedding增加的参数量会随序列语句长度线性增长。<strong>在可扩展性上</strong>，Learned Positional Embedding可扩展性较差，只能表征在max_*seq_*length以内的位置，而三角函数Position Encoding没有这样的限制，可扩展性更强。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoder</span>(d2l.Encoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;transformer编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span><br><span class="line"><span class="params">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, num_layers, dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                EncoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, use_bias))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, valid_lens, *args</span>):</span><br><span class="line">        <span class="comment"># 因为位置编码值在-1和1之间，</span></span><br><span class="line">        <span class="comment"># 因此嵌入值乘以嵌入维度的平方根进行缩放，</span></span><br><span class="line">        <span class="comment"># 然后再与位置编码相加。</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))<span class="comment">##嵌入层＋位置编码</span></span><br><span class="line">        self.attention_weights = [<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks)</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">            self.attention_weights[</span><br><span class="line">                i] = blk.attention.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>





<h3 id="decoder-1"><a href="#decoder-1" class="headerlink" title="decoder"></a>decoder</h3><p>在解码器&#x3D;&#x3D;自注意力&#x3D;&#x3D;中，查询、键和值都来自上一个解码器层的输出。但是，解码器中的每个位置只能考虑该位置之前的所有位置。这种<em>掩蔽</em>（masked）注意力保留了<em>自回归</em>（auto-regressive）属性，<strong>确保预测仅依赖于已生成的输出词元</strong>。关于<em>序列到序列模型</em>（sequence-to-sequence model），&#x3D;&#x3D;在训练阶段，其输出序列的所有位置（时间步）的词元都是已知的&#x3D;&#x3D;，可以通过sequence_mask来进行<strong>并行处理</strong>；然而，在预测阶段，其输出序列的词元是逐个生成的。因此，在任何解码器时间步中，只有生成的词元才能用于解码器的自注意力计算中。为了在解码器中保留自回归的属性，其掩蔽自注意力设定了参数<code>dec_valid_lens</code>，以便任何查询都只会与解码器中所有已经生成词元的位置（即直到该查询位置为止）进行注意力计算</p>
<p>解码器还在这两个子层之间插入了第三个子层，称为<em>编码器－解码器注意力</em>（encoder-decoder attention）层。 在编码器－解码器注意力中，<strong>查询来自前一个解码器层的输出</strong>，而键和值来自&#x3D;&#x3D;整个编码器的输出&#x3D;&#x3D;并非多头自注意力</p>
<p>&#x3D;&#x3D;自回归：用同一变量之前各期的表现情况，来预测该变量本期的表现情况，并假设它们为线性关系&#x3D;&#x3D;</p>
<p>Decoder端可以做并行化吗？</p>
<p>&#x3D;&#x3D;训练的时候可以，但是交互的时候不可以&#x3D;&#x3D;</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span><br><span class="line"><span class="params">                 dropout, i, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        self.attention1 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.attention2 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                                   num_hiddens)</span><br><span class="line">        self.addnorm3 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        enc_outputs, enc_valid_lens = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 训练阶段，输出序列的所有词元都在同一时间处理，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]初始化为None。</span></span><br><span class="line">        <span class="comment"># 预测阶段，输出序列是通过词元一个接着一个解码的，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示</span></span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), axis=<span class="number">1</span>)</span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, num_steps, _ = X.shape</span><br><span class="line">            <span class="comment"># dec_valid_lens的开头:(batch_size,num_steps),</span></span><br><span class="line">            <span class="comment"># 其中每一行是[1,2,...,num_steps]</span></span><br><span class="line">            dec_valid_lens = torch.arange(</span><br><span class="line">                <span class="number">1</span>, num_steps + <span class="number">1</span>, device=X.device).repeat(batch_size, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dec_valid_lens = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 自注意力</span></span><br><span class="line">        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)</span><br><span class="line">        Y = self.addnorm1(X, X2)</span><br><span class="line">        <span class="comment"># 编码器－解码器注意力。</span></span><br><span class="line">        <span class="comment"># enc_outputs的开头:(batch_size,num_steps,num_hiddens)</span></span><br><span class="line">        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">        Z = self.addnorm2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> self.addnorm3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure>


<p>训练的时候，1.&#x3D;&#x3D;初始decoder的time step为1时(也就是第一次接收输入)&#x3D;&#x3D;，其输入为一个特殊的token，可能是目标序列开始的token(如<BOS>)，也可能是源序列结尾的token(如<EOS>)，也可能是其它视任务而定的输入等等，不同源码中可能有微小的差异，其目标则是预测翻译后的第1个单词(token)是什么；2.然后<BOS>和预测出来的第1个单词一起，再次作为decoder的输入，得到第2个预测单词；3后续依此类推</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDecoder</span>(d2l.AttentionDecoder):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span><br><span class="line"><span class="params">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, num_layers, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                DecoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, i))</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_lens, [<span class="literal">None</span>] * self.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self._attention_weights = [[<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">            <span class="comment"># 解码器自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">0</span>][</span><br><span class="line">                i] = blk.attention1.attention.attention_weights</span><br><span class="line">            <span class="comment"># “编码器－解码器”自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">1</span>][</span><br><span class="line">                i] = blk.attention2.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure>

<p>基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP），这就是称前馈网络是<em>基于位置的</em>（positionwise）的原因</p>
<p>输入<code>X</code>的形状（批量大小，时间步数或序列长度，隐单元数或特征维度）将被一个两层的感知机转换成形状为（批量大小，时间步数，<code>ffn_num_outputs</code>）的输出张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFFN</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span></span><br><span class="line"><span class="params">                 **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.dense2(self.relu(self.dense1(X)))</span><br></pre></td></tr></table></figure>

<h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> exists</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn.functional <span class="keyword">import</span> log_softmax,pad</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> LambdaLR</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> altair <span class="keyword">as</span> alt <span class="comment">###绘图</span></span><br><span class="line"><span class="keyword">from</span> torchtext.data.functional <span class="keyword">import</span> to_map_style_dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> build_vocab_from_iterator</span><br><span class="line"><span class="keyword">import</span> torchtext.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">import</span> GPUtil</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.distributed <span class="keyword">import</span> DistributedSampler</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)<span class="comment">##忽略警告</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Define standard linear + softmax generation step.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> log_softmax(self.proj(x), dim=-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Construct a layernorm module (See citation for details).&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example_mask</span>():</span><br><span class="line">    LS_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;Subsequent Mask&quot;</span>: subsequent_mask(<span class="number">20</span>)[<span class="number">0</span>][x, y].flatten(),</span><br><span class="line">                    <span class="string">&quot;Window&quot;</span>: y,</span><br><span class="line">                    <span class="string">&quot;Masking&quot;</span>: x,</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(LS_data)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .properties(height=<span class="number">250</span>, width=<span class="number">250</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            alt.X(<span class="string">&quot;Window:O&quot;</span>),</span><br><span class="line">            alt.Y(<span class="string">&quot;Masking:O&quot;</span>),</span><br><span class="line">            alt.Color(<span class="string">&quot;Subsequent Mask:Q&quot;</span>, scale=alt.Scale(scheme=<span class="string">&quot;viridis&quot;</span>)),</span><br><span class="line">        )</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = scores.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implement the PE function.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(</span><br><span class="line">            torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * -(math.log(<span class="number">10000.0</span>) / d_model)</span><br><span class="line">        )</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&quot;pe&quot;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + self.pe[:, : x.size(<span class="number">1</span>)].requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_model</span>(<span class="params"></span></span><br><span class="line"><span class="params">    src_vocab, tgt_vocab, N=<span class="number">6</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;Helper: Construct a model from hyperparameters.&quot;</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This was important from their code.</span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform_(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TrainState</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Track number of steps, examples, and tokens processed&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    step: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># Steps in the current epoch</span></span><br><span class="line">    accum_step: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># Number of gradient accumulation steps</span></span><br><span class="line">    samples: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># total # of examples used</span></span><br><span class="line">    tokens: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># total # of tokens processed</span></span><br><span class="line">        </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_epoch</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data_iter,</span></span><br><span class="line"><span class="params">    model,</span></span><br><span class="line"><span class="params">    loss_compute,</span></span><br><span class="line"><span class="params">    optimizer,</span></span><br><span class="line"><span class="params">    scheduler,</span></span><br><span class="line"><span class="params">    mode=<span class="string">&quot;train&quot;</span>,</span></span><br><span class="line"><span class="params">    accum_iter=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">    train_state=TrainState(<span class="params"></span>),</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Train a single epoch&quot;&quot;&quot;</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    total_tokens = <span class="number">0</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    tokens = <span class="number">0</span></span><br><span class="line">    n_accum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_iter):</span><br><span class="line">        out = model.forward(</span><br><span class="line">            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask</span><br><span class="line">        )</span><br><span class="line">        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)</span><br><span class="line">        <span class="comment"># loss_node = loss_node / accum_iter</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&quot;train&quot;</span> <span class="keyword">or</span> mode == <span class="string">&quot;train+log&quot;</span>:</span><br><span class="line">            loss_node.backward()</span><br><span class="line">            train_state.step += <span class="number">1</span></span><br><span class="line">            train_state.samples += batch.src.shape[<span class="number">0</span>]<span class="comment">###batch数</span></span><br><span class="line">            train_state.tokens += batch.ntokens</span><br><span class="line">            <span class="keyword">if</span> i % accum_iter == <span class="number">0</span>:</span><br><span class="line">                optimizer.step()</span><br><span class="line">                optimizer.zero_grad(set_to_none=<span class="literal">True</span>)<span class="comment">##将梯度直接设置为none，减少内存使用</span></span><br><span class="line">                n_accum += <span class="number">1</span></span><br><span class="line">                train_state.accum_step += <span class="number">1</span></span><br><span class="line">            scheduler.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss</span><br><span class="line">        total_tokens += batch.ntokens</span><br><span class="line">        tokens += batch.ntokens</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">40</span> == <span class="number">1</span> <span class="keyword">and</span> (mode == <span class="string">&quot;train&quot;</span> <span class="keyword">or</span> mode == <span class="string">&quot;train+log&quot;</span>):</span><br><span class="line">            lr = optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">            elapsed = time.time() - start</span><br><span class="line">            <span class="built_in">print</span>(</span><br><span class="line">                (</span><br><span class="line">                    <span class="string">&quot;Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f &quot;</span></span><br><span class="line">                    + <span class="string">&quot;| Tokens / Sec: %7.1f | Learning Rate: %6.1e&quot;</span></span><br><span class="line">                )<span class="comment">###%e,指数形式</span></span><br><span class="line">                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)</span><br><span class="line">            )</span><br><span class="line">            start = time.time()</span><br><span class="line">            tokens = <span class="number">0</span></span><br><span class="line">        <span class="keyword">del</span> loss</span><br><span class="line">        <span class="keyword">del</span> loss_node</span><br><span class="line">    <span class="keyword">return</span> total_loss / total_tokens, train_state</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LabelSmoothing</span>(nn.Module):<span class="comment">####避免使用one-hot来表示真实标签</span></span><br><span class="line">    <span class="string">&quot;Implement label smoothing.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, padding_idx, smoothing=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(reduction=<span class="string">&quot;sum&quot;</span>)</span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, target</span>):</span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()<span class="comment">#batch,class</span></span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>))<span class="comment">#多填充了一个，但是padding会去掉</span></span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)<span class="comment">#在class上填confidence，target为batch中的真实标签的序号</span></span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span><span class="comment">#去掉一个class的可能性</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)<span class="comment">##是否有真实标签正好在这个去掉的class</span></span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)<span class="comment">#如果去掉了，那就直接不考虑这个样本了</span></span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, true_dist.clone().detach())</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_gen</span>(<span class="params">V, batch_size, nbatches</span>):</span><br><span class="line">    <span class="string">&quot;Generate random data for a src-tgt copy task.&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nbatches):</span><br><span class="line">        data = torch.randint(<span class="number">1</span>, V, size=(batch_size, <span class="number">10</span>))</span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        src = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        tgt = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_tokenizers</span>():</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        spacy_de = spacy.load(<span class="string">&quot;de_core_news_sm&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        os.system(<span class="string">&quot;python -m spacy download de_core_news_sm&quot;</span>)</span><br><span class="line">        spacy_de = spacy.load(<span class="string">&quot;de_core_news_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        spacy_en = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        os.system(<span class="string">&quot;python -m spacy download en_core_web_sm&quot;</span>)</span><br><span class="line">        spacy_en = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> spacy_de, spacy_en</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">text, tokenizer</span>):</span><br><span class="line">    <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> tokenizer.tokenizer(text)]<span class="comment">###分词器，将text分成词，也可以考虑颠倒顺序</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">yield_tokens</span>(<span class="params">data_iter, tokenizer, index</span>):<span class="comment">#生成器</span></span><br><span class="line">    <span class="keyword">for</span> from_to_tuple <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">yield</span> tokenizer(from_to_tuple[index])</span><br><span class="line">        </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_vocabulary</span>(<span class="params">spacy_de, spacy_en</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_de</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_de)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_en</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_en)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Building German Vocabulary ...&quot;</span>)</span><br><span class="line">    train, val, test = datasets.Multi30k(language_pair=(<span class="string">&quot;de&quot;</span>, <span class="string">&quot;en&quot;</span>))<span class="comment">#其train中每一个元素为两个句子组成，第一句为de第二句为en</span></span><br><span class="line">    vocab_src = build_vocab_from_iterator(</span><br><span class="line">        yield_tokens(train + val + test, tokenize_de, index=<span class="number">0</span>),</span><br><span class="line">        min_freq=<span class="number">2</span>,</span><br><span class="line">        specials=[<span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;/s&gt;&quot;</span>, <span class="string">&quot;&lt;blank&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Building English Vocabulary ...&quot;</span>)</span><br><span class="line">    train, val, test = datasets.Multi30k(language_pair=(<span class="string">&quot;de&quot;</span>, <span class="string">&quot;en&quot;</span>))</span><br><span class="line">    vocab_tgt = build_vocab_from_iterator(</span><br><span class="line">        yield_tokens(train + val + test, tokenize_en, index=<span class="number">1</span>),</span><br><span class="line">        min_freq=<span class="number">2</span>,</span><br><span class="line">        specials=[<span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;/s&gt;&quot;</span>, <span class="string">&quot;&lt;blank&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    vocab_src.set_default_index(vocab_src[<span class="string">&quot;&lt;unk&gt;&quot;</span>])</span><br><span class="line">    vocab_tgt.set_default_index(vocab_tgt[<span class="string">&quot;&lt;unk&gt;&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vocab_src, vocab_tgt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_vocab</span>(<span class="params">spacy_de, spacy_en</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> exists(<span class="string">&quot;vocab.pt&quot;</span>):</span><br><span class="line">        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)</span><br><span class="line">        torch.save((vocab_src, vocab_tgt), <span class="string">&quot;vocab.pt&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        vocab_src, vocab_tgt = torch.load(<span class="string">&quot;vocab.pt&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Finished.\nVocabulary sizes:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab_src))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab_tgt))</span><br><span class="line">    <span class="keyword">return</span> vocab_src, vocab_tgt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(</span><br><span class="line">        train_iter_map,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=(train_sampler <span class="keyword">is</span> <span class="literal">None</span>),</span><br><span class="line">        sampler=train_sampler,</span><br><span class="line">        collate_fn=collate_fn,<span class="comment">###对数据进行处理</span></span><br><span class="line">    )</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_batch</span>()<span class="comment">###返回填充后的batch数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">        <span class="keyword">return</span> collate_batch(</span><br><span class="line">            batch,</span><br><span class="line">            tokenize_de,</span><br><span class="line">            tokenize_en,</span><br><span class="line">            vocab_src,</span><br><span class="line">            vocab_tgt,</span><br><span class="line">            device,</span><br><span class="line">            max_padding=max_padding,</span><br><span class="line">            pad_id=vocab_src.get_stoi()[<span class="string">&quot;&lt;blank&gt;&quot;</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_worker</span>(<span class="params"></span></span><br><span class="line"><span class="params">    gpu,</span></span><br><span class="line"><span class="params">    ngpus_per_node,</span></span><br><span class="line"><span class="params">    vocab_src,</span></span><br><span class="line"><span class="params">    vocab_tgt,</span></span><br><span class="line"><span class="params">    spacy_de,</span></span><br><span class="line"><span class="params">    spacy_en,</span></span><br><span class="line"><span class="params">    config,</span></span><br><span class="line"><span class="params">    is_distributed=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Train worker process using GPU: <span class="subst">&#123;gpu&#125;</span> for training&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    torch.cuda.set_device(gpu)</span><br><span class="line"></span><br><span class="line">    pad_idx = vocab_tgt[<span class="string">&quot;&lt;blank&gt;&quot;</span>]</span><br><span class="line">    d_model = <span class="number">512</span></span><br><span class="line">    model = make_model(<span class="built_in">len</span>(vocab_src), <span class="built_in">len</span>(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.cuda(gpu)</span><br><span class="line">    module = model</span><br><span class="line">    is_main_process = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    criterion = LabelSmoothing(</span><br><span class="line">        size=<span class="built_in">len</span>(vocab_tgt), padding_idx=pad_idx, smoothing=<span class="number">0.1</span></span><br><span class="line">    )</span><br><span class="line">    criterion.cuda(gpu)</span><br><span class="line"></span><br><span class="line">    train_dataloader, valid_dataloader = create_dataloaders(</span><br><span class="line">        gpu,</span><br><span class="line">        vocab_src,</span><br><span class="line">        vocab_tgt,</span><br><span class="line">        spacy_de,</span><br><span class="line">        spacy_en,</span><br><span class="line">        batch_size=config[<span class="string">&quot;batch_size&quot;</span>] // ngpus_per_node,</span><br><span class="line">        max_padding=config[<span class="string">&quot;max_padding&quot;</span>],</span><br><span class="line">        is_distributed=is_distributed,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(</span><br><span class="line">        model.parameters(), lr=config[<span class="string">&quot;base_lr&quot;</span>], betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">    )</span><br><span class="line">    lr_scheduler = LambdaLR(</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        lr_lambda=<span class="keyword">lambda</span> step: rate(</span><br><span class="line">            step, d_model, factor=<span class="number">1</span>, warmup=config[<span class="string">&quot;warmup&quot;</span>]</span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line">    train_state = TrainState()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(config[<span class="string">&quot;num_epochs&quot;</span>]):</span><br><span class="line"></span><br><span class="line">        model.train()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[GPU<span class="subst">&#123;gpu&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> Training ====&quot;</span>, flush=<span class="literal">True</span>)<span class="comment">##print结束后立即清空内存，刷新显示</span></span><br><span class="line">        _, train_state = run_epoch(</span><br><span class="line">            (Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx) <span class="keyword">for</span> b <span class="keyword">in</span> train_dataloader),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(module.generator, criterion),</span><br><span class="line">            optimizer,</span><br><span class="line">            lr_scheduler,</span><br><span class="line">            mode=<span class="string">&quot;train+log&quot;</span>,</span><br><span class="line">            accum_iter=config[<span class="string">&quot;accum_iter&quot;</span>],</span><br><span class="line">            train_state=train_state,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        GPUtil.showUtilization()</span><br><span class="line">        <span class="keyword">if</span> is_main_process:</span><br><span class="line">            file_path = <span class="string">&quot;%s%.2d.pt&quot;</span> % (config[<span class="string">&quot;file_prefix&quot;</span>], epoch)</span><br><span class="line">            torch.save(module.state_dict(), file_path)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[GPU<span class="subst">&#123;gpu&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> Validation ====&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        sloss = run_epoch(</span><br><span class="line">            (Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx) <span class="keyword">for</span> b <span class="keyword">in</span> valid_dataloader),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(module.generator, criterion),</span><br><span class="line">            DummyOptimizer(),</span><br><span class="line">            DummyScheduler(),</span><br><span class="line">            mode=<span class="string">&quot;eval&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(sloss)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_main_process:</span><br><span class="line">        file_path = <span class="string">&quot;%sfinal.pt&quot;</span> % config[<span class="string">&quot;file_prefix&quot;</span>]</span><br><span class="line">        torch.save(module.state_dict(), file_path)  </span><br><span class="line">        </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_trained_model</span>():</span><br><span class="line">    config = &#123;</span><br><span class="line">        <span class="string">&quot;batch_size&quot;</span>: <span class="number">16</span>,</span><br><span class="line">        <span class="string">&quot;distributed&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">        <span class="string">&quot;num_epochs&quot;</span>: <span class="number">4</span>,</span><br><span class="line">        <span class="string">&quot;accum_iter&quot;</span>: <span class="number">10</span>,</span><br><span class="line">        <span class="string">&quot;base_lr&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">        <span class="string">&quot;max_padding&quot;</span>: <span class="number">72</span>,</span><br><span class="line">        <span class="string">&quot;warmup&quot;</span>: <span class="number">3000</span>,</span><br><span class="line">        <span class="string">&quot;file_prefix&quot;</span>: <span class="string">&quot;multi30k_model_&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    model_path = <span class="string">&quot;multi30k_model_final.pt&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> exists(model_path):</span><br><span class="line">        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)</span><br><span class="line"></span><br><span class="line">    model = make_model(<span class="built_in">len</span>(vocab_src), <span class="built_in">len</span>(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&quot;multi30k_model_final.pt&quot;</span>))</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mtx2df</span>(<span class="params">m, max_row, max_col, row_tokens, col_tokens</span>):</span><br><span class="line">    <span class="string">&quot;convert a dense matrix to a data frame with row and column indices&quot;</span></span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(</span><br><span class="line">        [</span><br><span class="line">            (</span><br><span class="line">                r,</span><br><span class="line">                c,</span><br><span class="line">                <span class="built_in">float</span>(m[r, c]),</span><br><span class="line">                <span class="string">&quot;%.3d %s&quot;</span></span><br><span class="line">                % (r, row_tokens[r] <span class="keyword">if</span> <span class="built_in">len</span>(row_tokens) &gt; r <span class="keyword">else</span> <span class="string">&quot;&lt;blank&gt;&quot;</span>),</span><br><span class="line">                <span class="string">&quot;%.3d %s&quot;</span></span><br><span class="line">                % (c, col_tokens[c] <span class="keyword">if</span> <span class="built_in">len</span>(col_tokens) &gt; c <span class="keyword">else</span> <span class="string">&quot;&lt;blank&gt;&quot;</span>),</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(m.shape[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(m.shape[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> r &lt; max_row <span class="keyword">and</span> c &lt; max_col</span><br><span class="line">        ],</span><br><span class="line">        <span class="comment"># if float(m[r,c]) != 0 and r &lt; max_row and c &lt; max_col],</span></span><br><span class="line">        columns=[<span class="string">&quot;row&quot;</span>, <span class="string">&quot;column&quot;</span>, <span class="string">&quot;value&quot;</span>, <span class="string">&quot;row_token&quot;</span>, <span class="string">&quot;col_token&quot;</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attn_map</span>(<span class="params">attn, layer, head, row_tokens, col_tokens, max_dim=<span class="number">30</span></span>):</span><br><span class="line">    df = mtx2df(</span><br><span class="line">        attn[<span class="number">0</span>, head].data,</span><br><span class="line">        max_dim,</span><br><span class="line">        max_dim,</span><br><span class="line">        row_tokens,</span><br><span class="line">        col_tokens,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(data=df)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .encode(</span><br><span class="line">            x=alt.X(<span class="string">&quot;col_token&quot;</span>, axis=alt.Axis(title=<span class="string">&quot;&quot;</span>)),</span><br><span class="line">            y=alt.Y(<span class="string">&quot;row_token&quot;</span>, axis=alt.Axis(title=<span class="string">&quot;&quot;</span>)),</span><br><span class="line">            color=<span class="string">&quot;value&quot;</span>,</span><br><span class="line">            tooltip=[<span class="string">&quot;row&quot;</span>, <span class="string">&quot;column&quot;</span>, <span class="string">&quot;value&quot;</span>, <span class="string">&quot;row_token&quot;</span>, <span class="string">&quot;col_token&quot;</span>],</span><br><span class="line">        )</span><br><span class="line">        .properties(height=<span class="number">400</span>, width=<span class="number">400</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_layer</span>(<span class="params">model, layer, getter_fn, ntokens, row_tokens, col_tokens</span>):</span><br><span class="line">    <span class="comment"># ntokens = last_example[0].ntokens</span></span><br><span class="line">    attn = getter_fn(model, layer)</span><br><span class="line">    n_heads = attn.shape[<span class="number">1</span>]</span><br><span class="line">    charts = [</span><br><span class="line">        attn_map(</span><br><span class="line">            attn,</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">            h,</span><br><span class="line">            row_tokens=row_tokens,</span><br><span class="line">            col_tokens=col_tokens,</span><br><span class="line">            max_dim=ntokens,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_heads)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">assert</span> n_heads == <span class="number">8</span></span><br><span class="line">    <span class="keyword">return</span> alt.vconcat(</span><br><span class="line">        charts[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># | charts[1]</span></span><br><span class="line">        | charts[<span class="number">2</span>]</span><br><span class="line">        <span class="comment"># | charts[3]</span></span><br><span class="line">        | charts[<span class="number">4</span>]</span><br><span class="line">        <span class="comment"># | charts[5]</span></span><br><span class="line">        | charts[<span class="number">6</span>]</span><br><span class="line">        <span class="comment"># | charts[7]</span></span><br><span class="line">        <span class="comment"># layer + 1 due to 0-indexing</span></span><br><span class="line">    ).properties(title=<span class="string">&quot;Layer %d&quot;</span> % (layer + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">viz_encoder_self</span>():</span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[</span><br><span class="line">        <span class="built_in">len</span>(example_data) - <span class="number">1</span></span><br><span class="line">    ]  <span class="comment"># batch object for the final example</span></span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model, layer, get_encoder, <span class="built_in">len</span>(example[<span class="number">1</span>]), example[<span class="number">1</span>], example[<span class="number">1</span>]</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># &amp; layer_viz[1]</span></span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        <span class="comment"># &amp; layer_viz[3]</span></span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        <span class="comment"># &amp; layer_viz[5]</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_encoder_self)</span><br></pre></td></tr></table></figure>





<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>独热向量不能准确表达不同词之间的&#x3D;&#x3D;相似度&#x3D;&#x3D;,因为对于两个独热向量，它们的余弦相似度为0</p>
<p>word2vec工具包含两个模型，即<em>跳元模型</em>（skip-gram） [<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#mikolov-sutskever-chen-ea-2013">Mikolov et al., 2013b]</a>和<em>连续词袋</em>（CBOW） [<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#mikolov-chen-corrado-ea-2013">Mikolov et al., 2013a]</a>。对于在语义上有意义的表示，它们的训练依赖于条件概率，条件概率可以被看作是使用语料库中一些词来预测另一些单词。由于是不带标签的数据，因此跳元模型和连续词袋都是自监督模型。</p>
<p>Word2vec的模型以大规模语料库作为输入，然后生成一个向量空间（通常为几百维）</p>
<p>Huffman树即最小带权路径的树，让权值小的叶子节点层次更深。</p>
<p>数据通信中，需要将传送的文字转换成二进制的字符串，用01的不同排列表示字符。二进制编码大致有两种方式：<strong>等长编码</strong>和<strong>变长编码</strong>。</p>
<p>等长编码即所有字符的编码长度相同，如果有6个字符，那么就需要3位二进制（）。由于等长编码对于所有字符的编码长度相同，因此&#x3D;&#x3D;对于一些出现频率极高的字符来说，等长编码会造成数据压缩率不高。&#x3D;&#x3D;</p>
<p>变长编码可以达到比等长编码好的多的压缩率，其思想就是&#x3D;&#x3D;<strong>赋予高频词短编码，低频词长编码</strong>&#x3D;&#x3D;。变长编码中我们只考虑『前缀编码』，即一个字符的编码不能是另一个字符编码的前缀。</p>
<p>因此，我们可以用字符集中的每个字符作为叶子节点生成一颗编码二叉树，为了获得传送报文的最短长度，可以将&#x3D;&#x3D;每个字符的出现频率作为字符节点的权值赋予该结点上&#x3D;&#x3D;，然后构造一棵Huffman树。利用Huffman树设计的二进制前缀编码，就被称为<strong>Huffman编码</strong>。</p>
<p>Word2vec算法也用了Huffman编码，它把训练语料中的词当成叶子节点，其在语料中出现的次数当做权值，通过构造响应的Huffman树来对每一个词进行Huffman编码</p>
<p>Distributed Representation:每个词映射为固定长度的短向量。通过刻画两个向量之间的距离来刻画两个向量之间的相似度,从高维映射到低维</p>
<p>N-gram模型是存储下来所有可能的概率参数，然后计算时对概率进行连乘。机器学习领域有一种较为通用的做法：<strong>对所考虑的问题建模后，先为其构造一个目标函数进行优化，求得一组最优参数，然后用最优参数对应的模型来预测</strong>。对于统计语言模型来说，我们通常构造的目标函数是『最大似然函数』。由于连乘可能导致概率极小，所以经常采用的是<strong>『最大对数似然』</strong></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220718204800931.png" alt="image-20220718204800931"></p>
<p>注意：在对做$$y_w$$Softmax归一化之后，对应的分量就表示当前词是w的概率。</p>
<p>相比于N-gram模型来说，神经概率语言模型有如下优点：</p>
<ol>
<li>词语与词语间的&#x3D;&#x3D;相似度可以通过词向量&#x3D;&#x3D;来体现</li>
<li>基于词向量的模型自带&#x3D;&#x3D;『平滑化』&#x3D;&#x3D;功能，无需额外处理</li>
</ol>
<p>主要的缺点就是<strong>计算量太大</strong><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220718205035765.png" alt="image-20220718205035765"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220718205103166.png" alt="image-20220718205103166"></p>
<h3 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip-gram"></a>skip-gram</h3><p>跳元模型假设一个词可以用来在文本序列中生成其周围的单词,跳元模型考虑了在给定中心词的情况下生成周围上下文词的条件概率,</p>
<p>假设这个词在词典中索引为$i$，当它为中心词时向量表示为$\boldsymbol{v}_i\in\mathbb{R}^d$，而为背景词时向量表示为$\boldsymbol{u}_i\in\mathbb{R}^d$。设中心词$w_c$在词典中索引为$c$，背景词$w_o$在词典中索引为$o$，给定中心词生成背景词的条件概率可以通过&#x3D;&#x3D;对向量内积做softmax&#x3D;&#x3D;运算而得到：</p>
<p>$$P(w_o \mid w_c) &#x3D; \frac{\text{exp}(\boldsymbol{u}_o^\top \boldsymbol{v}<em>c)}{ \sum</em>{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)},$$</p>
<p>其中词典索引集$\mathcal{V} &#x3D; {0, 1, \ldots, |\mathcal{V}|-1}$。</p>
<p>假设给定一个长度为$T$的文本序列，设时间步$t$的词为$w^{(t)}$。假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为$m$时，跳字模型的&#x3D;&#x3D;似然函数即给定任一中心词生成所有背景词的概率&#x3D;&#x3D;</p>
<p>$$ \prod_{t&#x3D;1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} P(w^{(t+j)} \mid w^{(t)}),$$</p>
<p>这里小于1和大于$T$的时间步可以忽略。</p>
<p>训练中我们通过最大化似然函数来学习模型参数，即最大似然估计。这等价于最小化以下损失函数：</p>
<p>$$ - \sum_{t&#x3D;1}^{T} \sum_{-m \leq j \leq m,\ j \neq 0} \text{log}, P(w^{(t+j)} \mid w^{(t)}).$$</p>
<p>如果使用随机梯度下降，那么在每一次迭代里我们随机采样一个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数。梯度计算的关键是条件概率的对数有关中心词向量和背景词向量的梯度。根据定义，首先看到</p>
<p>$$\log P(w_o \mid w_c) &#x3D;<br>\boldsymbol{u}_o^\top \boldsymbol{v}<em>c - \log\left(\sum</em>{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)\right)$$</p>
<p>通过微分，我们可以得到上式中$\boldsymbol{v}_c$的梯度</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial \text{log}, P(w_o \mid w_c)}{\partial \boldsymbol{v}_c}<br>&amp;&#x3D; \boldsymbol{u}<em>o - \frac{\sum</em>{j \in \mathcal{V}} \exp(\boldsymbol{u}_j^\top \boldsymbol{v}_c)\boldsymbol{u}<em>j}{\sum</em>{i \in \mathcal{V}} \exp(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}\<br>&amp;&#x3D; \boldsymbol{u}<em>o - \sum</em>{j \in \mathcal{V}} \left(\frac{\text{exp}(\boldsymbol{u}_j^\top \boldsymbol{v}<em>c)}{ \sum</em>{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}\right) \boldsymbol{u}_j\<br>&amp;&#x3D; \boldsymbol{u}<em>o - \sum</em>{j \in \mathcal{V}} P(w_j \mid w_c) \boldsymbol{u}_j.<br>\end{aligned}<br>$$</p>
<p>&#x3D;&#x3D;它的计算需要词典中所有词以$w_c$为中心词的条件概率。有关其他词向量的梯度同理可得。&#x3D;&#x3D;</p>
<p>训练结束后，对于词典中的任一索引为$i$的词，我们均得到该词作为中心词和背景词的两组词向量$\boldsymbol{v}_i$和$\boldsymbol{u}_i$。在自然语言处理应用中，一般使用跳字模型的&#x3D;&#x3D;中心词向量作为词的表征向量&#x3D;&#x3D;。</p>
<h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p>与跳字模型最大的不同在于，连续词袋模型假设基于某中心词在文本序列前后的背景词来生成该中心词。</p>
<p>因为连续词袋模型的背景词有多个，&#x3D;&#x3D;我们将这些背景词向量取平均&#x3D;&#x3D;，然后使用和跳字模型一样的方法来计算条件概率。设$\boldsymbol{v_i}\in\mathbb{R}^d$和$\boldsymbol{u_i}\in\mathbb{R}^d$分别表示词典中索引为$i$的词作为背景词和中心词的向量（注意符号的含义与跳字模型中的相反）。设中心词$w_c$在词典中索引为$c$，背景词$w_{o_1}, \ldots, w_{o_{2m}}$在词典中索引为$o_1, \ldots, o_{2m}$，那么给定背景词生成中心词的条件概率</p>
<p>$$P(w_c \mid w_{o_1}, \ldots, w_{o_{2m}}) &#x3D; \frac{\text{exp}\left(\frac{1}{2m}\boldsymbol{u}<em>c^\top (\boldsymbol{v}</em>{o_1} + \ldots + \boldsymbol{v}<em>{o</em>{2m}}) \right)}{ \sum_{i \in \mathcal{V}} \text{exp}\left(\frac{1}{2m}\boldsymbol{u}<em>i^\top (\boldsymbol{v}</em>{o_1} + \ldots + \boldsymbol{v}<em>{o</em>{2m}}) \right)}.$$</p>
<p>为了让符号更加简单，我们记$\mathcal{W}<em>o&#x3D; {w</em>{o_1}, \ldots, w_{o_{2m}}}$，且$\bar{\boldsymbol{v}}<em>o &#x3D; \left(\boldsymbol{v}</em>{o_1} + \ldots + \boldsymbol{v}<em>{o</em>{2m}} \right)&#x2F;(2m)$，那么上式可以简写成</p>
<p>$$P(w_c \mid \mathcal{W}_o) &#x3D; \frac{\exp\left(\boldsymbol{u}_c^\top \bar{\boldsymbol{v}}<em>o\right)}{\sum</em>{i \in \mathcal{V}} \exp\left(\boldsymbol{u}_i^\top \bar{\boldsymbol{v}}_o\right)}.$$</p>
<p>给定一个长度为$T$的文本序列，设时间步$t$的词为$w^{(t)}$，背景窗口大小为$m$。连续词袋模型的似然函数是由背景词生成任一中心词的概率</p>
<p>$$ \prod_{t&#x3D;1}^{T}  P(w^{(t)} \mid  w^{(t-m)}, \ldots,  w^{(t-1)},  w^{(t+1)}, \ldots,  w^{(t+m)}).$$</p>
<p>训练连续词袋模型同训练跳字模型基本一致。连续词袋模型的最大似然估计等价于最小化损失函数</p>
<p>$$  -\sum_{t&#x3D;1}^T  \text{log}, P(w^{(t)} \mid  w^{(t-m)}, \ldots,  w^{(t-1)},  w^{(t+1)}, \ldots,  w^{(t+m)}).$$</p>
<p>注意到</p>
<p>$$\log,P(w_c \mid \mathcal{W}_o) &#x3D; \boldsymbol{u}_c^\top \bar{\boldsymbol{v}}<em>o - \log,\left(\sum</em>{i \in \mathcal{V}} \exp\left(\boldsymbol{u}_i^\top \bar{\boldsymbol{v}}_o\right)\right).$$</p>
<p>通过微分，我们可以计算出上式中条件概率的对数有关任一背景词向量$\boldsymbol{v}_{o_i}$（$i &#x3D; 1, \ldots, 2m$）的梯度</p>
<p>$$\frac{\partial \log, P(w_c \mid \mathcal{W}<em>o)}{\partial \boldsymbol{v}</em>{o_i}} &#x3D; \frac{1}{2m} \left(\boldsymbol{u}<em>c - \sum</em>{j \in \mathcal{V}} \frac{\exp(\boldsymbol{u}_j^\top \bar{\boldsymbol{v}}_o)\boldsymbol{u}<em>j}{ \sum</em>{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \bar{\boldsymbol{v}}_o)} \right) &#x3D; \frac{1}{2m}\left(\boldsymbol{u}<em>c - \sum</em>{j \in \mathcal{V}} P(w_j \mid \mathcal{W}_o) \boldsymbol{u}_j \right).$$</p>
<p>有关其他词向量的梯度同理可得。同跳字模型不一样的一点在于，我们一般使用连续词袋模型的&#x3D;&#x3D;背景词向量&#x3D;&#x3D;作为词的表征向量。</p>
<h3 id="approximate-training"><a href="#approximate-training" class="headerlink" title="approximate training"></a>approximate training</h3><p>论是跳字模型还是连续词袋模型，由于条件概率&#x3D;&#x3D;使用了softmax运算&#x3D;&#x3D;，每一步的梯度计算都包含词典大小数目的项的累加。对于含几十万或上百万词的较大词典，每次的&#x3D;&#x3D;梯度计算开销可能过大&#x3D;&#x3D;。为了降低该计算复杂度，本节将介绍两种近似训练方法，即负采样（negative sampling）或层序softmax（hierarchical softmax）.</p>
<ul>
<li><p>层序softmax Hierarchical Softmax</p>
<p>CBOW模型全名为continous bag-of-words。之所以叫是因为输入层到投影层的操作由『拼接』变成了『叠加』，对于『叠加而言』，无所谓词的顺序，所以称为词袋</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220721104620656.png" alt="image-20220721104620656"><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220721104712686.png" alt="image-20220721104712686"></p>
</li>
</ul>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220718212844039.png" alt="image-20220718212844039"></p>
<p>&#x3D;&#x3D;skip-gram&#x3D;&#x3D;<br><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220721104827813.png" alt="image-20220721104827813"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220721104906610.png" alt="image-20220721104906610"></p>
<ul>
<li><p>负采样</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220719123252265.png" alt="image-20220719123252265">选取负样本需要按照一定的概率分布，Word2vec的作者们测试发现<strong>最佳的分布是3&#x2F;4次幂的</strong>unigram distribution（一元分布）<img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220719114855235.png" alt="image-20220719114855235"></p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220719123336259.png" alt="image-20220719123336259"><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220719123402334.png" alt="image-20220719123402334">修修改目标函数：</p>
<p>&#x3D;&#x3D;每个训练步的梯度计算成本与词表大小无关，而是线性依赖于负采样集合大小&#x3D;&#x3D;</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220719123649115.png" alt="image-20220719123649115"></p>
<p>任意上下文词$$w_0$$来自该上下文窗口的被认为是由下式建模概率的事件:</p>
<p>$$P(D&#x3D;1\mid w_c, w_o) &#x3D; \sigma(\boldsymbol{u}_o^\top \boldsymbol{v}_c),$$</p>
<p>负采样修改了原来的目标函数。给定中心词$w_c$的一个背景窗口，我们把背景词$w_o$出现在该背景窗口看作一个事件，并将该事件的概率计算为</p>
<p>$$P(D&#x3D;1\mid w_c, w_o) &#x3D; \sigma(\boldsymbol{u}_o^\top \boldsymbol{v}_c),$$</p>
<p>其中的$\sigma$函数与sigmoid激活函数的定义相同：</p>
<p>$$\sigma(x) &#x3D; \frac{1}{1+\exp(-x)}.$$</p>
<p>我们先考虑最大化文本序列中所有该事件的联合概率来训练词向量。具体来说，给定一个长度为$T$的文本序列，设时间步$t$的词为$w^{(t)}$且背景窗口大小为$m$，考虑最大化联合概率</p>
<p>$$ \prod_{t&#x3D;1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} P(D&#x3D;1\mid w^{(t)}, w^{(t+j)}).$$</p>
<p>然而，以上模型中包含的事件仅考虑了正类样本。这导致当所有词向量相等且值为无穷大时，以上的联合概率才被最大化为1。很明显，这样的词向量毫无意义。负采样通过采样并添加负类样本使目标函数更有意义。设背景词$w_o$出现在中心词$w_c$的一个背景窗口为事件$P$，我们根据分布$P(w)$采样$K$个未出现在该背景窗口中的词，即噪声词。设噪声词$w_k$（$k&#x3D;1, \ldots, K$）不出现在中心词$w_c$的该背景窗口为事件$N_k$。假设同时含有正类样本和负类样本的事件$P, N_1, \ldots, N_K$相互独立，负采样将以上需要最大化的仅考虑正类样本的联合概率改写为</p>
<p>$$ \prod_{t&#x3D;1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} P(w^{(t+j)} \mid w^{(t)}),$$</p>
<p>其中条件概率被近似表示为<br>$$ P(w^{(t+j)} \mid w^{(t)}) &#x3D;P(D&#x3D;1\mid w^{(t)}, w^{(t+j)})\prod_{k&#x3D;1,\ w_k \sim P(w)}^K P(D&#x3D;0\mid w^{(t)}, w_k).$$</p>
<p>设文本序列中时间步$t$的词$w^{(t)}$在词典中的索引为$i_t$，噪声词$w_k$在词典中的索引为$h_k$。有关以上条件概率的对数损失为</p>
<p>$$<br>\begin{aligned}<br>-\log P(w^{(t+j)} \mid w^{(t)})<br>&#x3D;&amp; -\log P(D&#x3D;1\mid w^{(t)}, w^{(t+j)}) - \sum_{k&#x3D;1,\ w_k \sim P(w)}^K \log P(D&#x3D;0\mid w^{(t)}, w_k)\<br>&#x3D;&amp;-  \log, \sigma\left(\boldsymbol{u}<em>{i</em>{t+j}}^\top \boldsymbol{v}<em>{i_t}\right) - \sum</em>{k&#x3D;1,\ w_k \sim P(w)}^K \log\left(1-\sigma\left(\boldsymbol{u}<em>{h_k}^\top \boldsymbol{v}</em>{i_t}\right)\right)\<br>&#x3D;&amp;-  \log, \sigma\left(\boldsymbol{u}<em>{i</em>{t+j}}^\top \boldsymbol{v}<em>{i_t}\right) - \sum</em>{k&#x3D;1,\ w_k \sim P(w)}^K \log\sigma\left(-\boldsymbol{u}<em>{h_k}^\top \boldsymbol{v}</em>{i_t}\right).<br>\end{aligned}<br>$$</p>
<p>现在，训练中每一步的梯度计算开销不再与词典大小相关，而与$K$线性相关。当$K$取较小的常数时，负采样在每一步的梯度计算开销较小。</p>
</li>
</ul>
<ul>
<li><p>文本数据中一般会出现一些高频词，如英文中的“the”“a”和“in”。通常来说，在一个背景窗口中，一个词（如“chip”）和较低频词（如“microprocessor”）同时出现比和较高频词（如“the”）同时出现对训练词嵌入模型更有益。因此，训练词嵌入模型时可以对词进行二次采样 [2]。<br>具体来说，数据集中每个被索引词$w_i$将有一定概率被丢弃，该丢弃概率为<br>$$<br>P(w_i) &#x3D; \max\left(1 - \sqrt{\frac{t}{f(w_i)}}, 0\right),<br>$$</p>
<p>其中 $f(w_i)$ 是数据集中词$w_i$的个数与总词数之比，常数$t$是一个超参数（实验中设为$10^{-4}$）。可见，只有当$f(w_i) &gt; t$时，我们才有可能在二次采样中丢弃词$w</p>
</li>
</ul>
<p>我们使用负采样来进行近似训练。对于一对中心词和背景词，我们随机采样$K$个噪声词（实验中设$K&#x3D;5$）。根据word2vec论文的建议，噪声词采样概率$P(w)$设为$w$词频与总词频之比的0.75次方 [2]。   </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_negatives</span>(<span class="params">all_contexts, sampling_weights, K</span>):</span><br><span class="line">    all_negatives, neg_candidates, i = [], [], <span class="number">0</span></span><br><span class="line">    population = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(sampling_weights)))</span><br><span class="line">    <span class="keyword">for</span> contexts <span class="keyword">in</span> all_contexts:</span><br><span class="line">        negatives = []</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(negatives) &lt; <span class="built_in">len</span>(contexts) * K:</span><br><span class="line">            <span class="keyword">if</span> i == <span class="built_in">len</span>(neg_candidates):</span><br><span class="line">                <span class="comment"># 根据每个词的权重（sampling_weights）随机生成k个词的索引作为噪声词。</span></span><br><span class="line">                <span class="comment"># 为了高效计算，可以将k设得稍大一点</span></span><br><span class="line">                i, neg_candidates = <span class="number">0</span>, random.choices(</span><br><span class="line">                    population, sampling_weights, k=<span class="built_in">int</span>(<span class="number">1e5</span>))</span><br><span class="line">            neg, i = neg_candidates[i], i + <span class="number">1</span></span><br><span class="line">            <span class="comment"># 噪声词不能是背景词</span></span><br><span class="line">            <span class="keyword">if</span> neg <span class="keyword">not</span> <span class="keyword">in</span> <span class="built_in">set</span>(contexts):</span><br><span class="line">                negatives.append(neg)</span><br><span class="line">        all_negatives.append(negatives)</span><br><span class="line">    <span class="keyword">return</span> all_negatives</span><br><span class="line"></span><br><span class="line">sampling_weights = [counter[w]**<span class="number">0.75</span> <span class="keyword">for</span> w <span class="keyword">in</span> idx_to_token]</span><br><span class="line">all_negatives = get_negatives(all_contexts, sampling_weights, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<p>我们从数据集中提取所有中心词<code>all_centers</code>，以及每个中心词对应的背景词<code>all_contexts</code>和噪声词<code>all_negatives</code>。我们先定义一个<code>Dataset</code>类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, centers, contexts, negatives</span>):</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(centers) == <span class="built_in">len</span>(contexts) == <span class="built_in">len</span>(negatives)</span><br><span class="line">        self.centers = centers</span><br><span class="line">        self.contexts = contexts</span><br><span class="line">        self.negatives = negatives</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> (self.centers[index], self.contexts[index], self.negatives[index])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.centers)</span><br></pre></td></tr></table></figure>

<p>我们将通过随机小批量来读取它们。在一个小批量数据中，第$i$个样本包括一个中心词以及它所对应的$n_i$个背景词和$m_i$个噪声词。由于每个样本的背景窗口大小可能不一样，其中背景词与噪声词个数之和$n_i+m_i$也会不同。在构造小批量时，我们将每个样本的背景词和噪声词连结在一起，并添加填充项0直至连结后的长度相同，即长度均为$\max_i n_i+m_i$（<code>max_len</code>变量）。为了避免填充项对损失函数计算的影响，我们构造了掩码变量<code>masks</code>，其每一个元素分别与连结后的背景词和噪声词<code>contexts_negatives</code>中的元素一一对应。当<code>contexts_negatives</code>变量中的某个元素为填充项时，相同位置的掩码变量<code>masks</code>中的元素取0，否则取1。为了区分正类和负类，我们还需要将<code>contexts_negatives</code>变量中的背景词和噪声词区分开来。依据掩码变量的构造思路，我们只需创建与<code>contexts_negatives</code>变量形状相同的标签变量<code>labels</code>，并将与背景词（正类）对应的元素设1，其余清0。</p>
<p>下面我们实现这个小批量读取函数<code>batchify</code>。它的小批量输入<code>data</code>是一个长度为批量大小的列表，其中每个元素分别包含中心词<code>center</code>、背景词<code>context</code>和噪声词<code>negative</code>。该函数返回的小批量数据符合我们需要的格式，例如，包含了掩码变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batchify</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用作DataLoader的参数collate_fn: 输入是个长为batchsize的list, </span></span><br><span class="line"><span class="string">    list中的每个元素都是Dataset类调用__getitem__得到的结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    max_len = <span class="built_in">max</span>(<span class="built_in">len</span>(c) + <span class="built_in">len</span>(n) <span class="keyword">for</span> _, c, n <span class="keyword">in</span> data)</span><br><span class="line">    centers, contexts_negatives, masks, labels = [], [], [], []</span><br><span class="line">    <span class="keyword">for</span> center, context, negative <span class="keyword">in</span> data:</span><br><span class="line">        cur_len = <span class="built_in">len</span>(context) + <span class="built_in">len</span>(negative)</span><br><span class="line">        centers += [center]</span><br><span class="line">        contexts_negatives += [context + negative + [<span class="number">0</span>] * (max_len - cur_len)]</span><br><span class="line">        masks += [[<span class="number">1</span>] * cur_len + [<span class="number">0</span>] * (max_len - cur_len)]</span><br><span class="line">        labels += [[<span class="number">1</span>] * <span class="built_in">len</span>(context) + [<span class="number">0</span>] * (max_len - <span class="built_in">len</span>(context))]</span><br><span class="line">    <span class="keyword">return</span> (torch.tensor(centers).view(-<span class="number">1</span>, <span class="number">1</span>), torch.tensor(contexts_negatives),</span><br><span class="line">            torch.tensor(masks), torch.tensor(labels))</span><br></pre></td></tr></table></figure>

<p>我们用刚刚定义的<code>batchify</code>函数指定<code>DataLoader</code>实例中小批量的读取方式，然后打印读取的第一个批量中各个变量的形状。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">512</span></span><br><span class="line">num_workers = <span class="number">0</span> <span class="keyword">if</span> sys.platform.startswith(<span class="string">&#x27;win32&#x27;</span>) <span class="keyword">else</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">dataset = MyDataset(all_centers, </span><br><span class="line">                    all_contexts, </span><br><span class="line">                    all_negatives)</span><br><span class="line">data_iter = Data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            collate_fn=batchify, </span><br><span class="line">                            num_workers=num_workers)</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">    <span class="keyword">for</span> name, data <span class="keyword">in</span> <span class="built_in">zip</span>([<span class="string">&#x27;centers&#x27;</span>, <span class="string">&#x27;contexts_negatives&#x27;</span>, <span class="string">&#x27;masks&#x27;</span>,</span><br><span class="line">                           <span class="string">&#x27;labels&#x27;</span>], batch):</span><br><span class="line">        <span class="built_in">print</span>(name, <span class="string">&#x27;shape:&#x27;</span>, data.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">centers shape: torch.Size([512, 1])</span><br><span class="line">contexts_negatives shape: torch.Size([512, 60])</span><br><span class="line">masks shape: torch.Size([512, 60])</span><br><span class="line">labels shape: torch.Size([512, 60])</span><br></pre></td></tr></table></figure>

<p>下面定义训练函数。由于填充项的存在，与之前的训练函数相比，损失函数的计算稍有不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, lr, num_epochs</span>):</span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;train on&quot;</span>, device)</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        start, l_sum, n = time.time(), <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">            center, context_negative, mask, label = [d.to(device) <span class="keyword">for</span> d <span class="keyword">in</span> batch]</span><br><span class="line">            </span><br><span class="line">            pred = skip_gram(center, context_negative, net[<span class="number">0</span>], net[<span class="number">1</span>])</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 使用掩码变量mask来避免填充项对损失函数计算的影响</span></span><br><span class="line">            l = (loss(pred.view(label.shape), label, mask) *</span><br><span class="line">                 mask.shape[<span class="number">1</span>] / mask.<span class="built_in">float</span>().<span class="built_in">sum</span>(dim=<span class="number">1</span>)).mean() <span class="comment"># 一个batch的平均loss</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            l_sum += l.cpu().item()</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %.2f, time %.2fs&#x27;</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, l_sum / n, time.time() - start))</span><br></pre></td></tr></table></figure>

<p>现在我们就可以使用负采样训练跳字模型了。</p>
<h2 id="Subword-embedding"><a href="#Subword-embedding" class="headerlink" title="Subword embedding"></a>Subword embedding</h2><p>在word2vec中，我们并没有直接利用构词学中的信息。无论是在跳字模型还是连续词袋模型中，我们都将形态不同的单词用不同的向量来表示。例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。鉴于此，fastText提出了子词嵌入（subword embedding）的方法，从而试图将构词信息引入word2vec中的跳字模型 [1]。</p>
<p>在单词的首尾分别添加特殊字符“&lt;”和“&gt;”以区分作为前后缀的子词。然后，将单词当成一个由字符构成的序列来提取$n$元语法。例如，当$n&#x3D;3$时，我们得到所有长度为3的子词：“&lt;wh&gt;”“whe”“her”“ere”“&lt;re&gt;”以及特殊子词“&lt;where&gt;”。</p>
<p>在fastText中，对于一个词$w$，我们将它所有长度在$3 \sim 6$的子词和特殊子词的并集记为$\mathcal{G}_w$。那么词典则是所有词的子词集合的并集。假设词典中子词$g$的向量为$\boldsymbol{z}_g$，那么跳字模型中词$w$的作为中心词的向量$\boldsymbol{v}_w$则表示成</p>
<p>$$<br>\boldsymbol{v}<em>w &#x3D; \sum</em>{g\in\mathcal{G}_w} \boldsymbol{z}_g.<br>$$</p>
<p>fastText的其余部分同跳字模型一致，不在此重复。可以看到，与跳字模型相比，fastText中词典规模更大，造成模型参数更多，同时一个词的向量需要对所有子词向量求和，继而导致计算复杂度更高。但与此同时，&#x3D;&#x3D;较生僻的复杂单词，甚至是词典中没有的单词&#x3D;&#x3D;，可能会从同它结构类似的其他词那里获取更好的词向量表示。</p>
<h2 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h2><ul>
<li>诸如词-词共现计数的全局语料库统计可以来解释跳元模型。</li>
<li>交叉熵损失可能不是衡量两种概率分布差异的好选择，特别是对于大型语料库。GloVe使用平方损失来拟合预先计算的全局语料库统计数据。</li>
<li>对于GloVe中的任意词，&#x3D;&#x3D;中心词向量和上下文词向量在数学上是等价的&#x3D;&#x3D;。</li>
<li>GloVe可以从词-词共现概率的比率来解释。</li>
</ul>
<p>将跳字模型中使用softmax运算表达的条件概率$P(w_j\mid w_i)$记作$q_{ij}$，即<br>$$<br>q_{ij}&#x3D;\frac{\exp(\boldsymbol{u}_j^\top \boldsymbol{v}<em>i)}{ \sum</em>{k \in \mathcal{V}} \text{exp}(\boldsymbol{u}_k^\top \boldsymbol{v}_i)},<br>$$</p>
<p>其中$\boldsymbol{v}_i$和$\boldsymbol{u}_i$分别是索引为$i$的词$w_i$作为中心词和背景词时的向量表示，$\mathcal{V} &#x3D; {0, 1, \ldots, |\mathcal{V}|-1}$为词典索引集。</p>
<p>对于词$w_i$，它在数据集中可能多次出现。我们将每一次以它作为中心词的所有背景词全部汇总并保留重复元素，记作多重集（multiset）$\mathcal{C}_i$。一个元素在多重集中的个数称为该元素的重数（multiplicity）。举例来说，假设词$w_i$在数据集中出现2次：文本序列中以这2个$w_i$作为中心词的背景窗口分别包含背景词索引$2,1,5,2$和$2,3,2,1$。那么多重集$$\mathcal{C}_i &#x3D; {1,1,2,2,2,2,3,5}$$</p>
<p>将多重集$\mathcal{C}<em>i$中元素$j$的重数记作$x</em>{ij}$：它表示了整个数据集中所有以$w_i$为中心词的背景窗口中词$w_j$的个数。那么，跳字模型的损失函数还可以用另一种方式表达：</p>
<p>$$<br>-\sum_{i\in\mathcal{V}}\sum_{j\in\mathcal{V}} x_{ij} \log,q_{ij}.<br>$$</p>
<p>我们将数据集中所有以词$w_i$为中心词的背景词的数量之和$\left|\mathcal{C}<em>i\right|$记为$x_i$，并将以$w_i$为中心词生成背景词$w_j$的条件概率$x</em>{ij}&#x2F;x_i$记作$p_{ij}$。我们可以进一步改写跳字模型的损失函数为</p>
<p>$$<br>-\sum_{i\in\mathcal{V}} x_i \sum_{j\in\mathcal{V}} p_{ij} \log,q_{ij}.<br>$$</p>
<p>上式中，$-\sum_{j\in\mathcal{V}} p_{ij} \log,q_{ij}$计算的是以$w_i$为中心词的背景词条件概率分布$p_{ij}$和模型预测的条件概率分布$q_{ij}$的交叉熵，且损失函数使用所有以词$w_i$为中心词的背景词的数量之和来加权。最小化上式中的损失函数会令预测的条件概率分布尽可能接近真实的条件概率分布。</p>
<p>令模型预测$q_{ij}$成为合法概率分布的代价是它在分母中基于整个词典的累加项。这很容易带来过大的计算开销。另一方面，词典中往往有大量生僻词，它们在数据集中出现的次数极少。而&#x3D;&#x3D;有关大量生僻词的条件概率分布在交叉熵损失函数中的最终预测往往并不准确。&#x3D;&#x3D;</p>
<p>鉴于此，作为在word2vec之后提出的词嵌入模型，GloVe模型采用了平方损失，并基于该损失对跳字模型做了3点改动 [1]：</p>
<ol>
<li>&#x3D;&#x3D;使用非概率分布的变量$p’<em>{ij}&#x3D;x</em>{ij}$和$q’_{ij}&#x3D;\exp(\boldsymbol{u}<em>j^\top \boldsymbol{v}<em>i)$，并对它们取对数。因此，平方损失项是$\left(\log,p’</em>{ij} - \log,q’</em>{ij}\right)^2 &#x3D; \left(\boldsymbol{u}_j^\top \boldsymbol{v}<em>i - \log,x</em>{ij}\right)^2$。&#x3D;&#x3D;</li>
<li>为每个词$w_i$增加两个为标量的模型参数：中心词偏差项$b_i$和背景词偏差项$c_i$。</li>
<li>将每个损失项的权重替换成函数$h(x_{ij})$。权重函数$h(x)$是值域在$[0,1]$的单调递增函数。</li>
</ol>
<p>&#x3D;&#x3D;如此一来，GloVe模型的目标是最小化损失函数&#x3D;&#x3D;</p>
<p>$$\sum_{i\in\mathcal{V}} \sum_{j\in\mathcal{V}} h(x_{ij}) \left(\boldsymbol{u}_j^\top \boldsymbol{v}<em>i + b_i + c_j - \log,x</em>{ij}\right)^2.$$</p>
<p>其中权重函数$h(x)$的一个建议选择是：当$x &lt; c$时（如$c &#x3D; 100$），令$h(x) &#x3D; (x&#x2F;c)^\alpha$（如$\alpha &#x3D; 0.75$），反之令$h(x) &#x3D; 1$。因为$h(0)&#x3D;0$，所以对于$x_{ij}&#x3D;0$的平方损失项可以直接忽略。&#x3D;&#x3D;当使用小批量随机梯度下降来训练时&#x3D;&#x3D;，每个时间步我们随机采样小批量非零$x_{ij}$，然后计算梯度来迭代模型参数。这些非零$x_{ij}$是预先基于整个数据集计算得到的，&#x3D;&#x3D;包含了数据集的全局统计信息&#x3D;&#x3D;。因此，GloVe模型的命名取“全局向量”（Global Vectors）之意。</p>
<p>需要强调的是，如果词$w_i$出现在词$w_j$的背景窗口里，那么词$w_j$也会出现在词$w_i$的背景窗口里。也就是说，$x_{ij}&#x3D;x_{ji}$。不同于word2vec中拟合的是非对称的条件概率$p_{ij}$，GloVe模型拟合的是对称的$\log, x_{ij}$。因此，任意词的中心词向量和背景词向量在GloVe模型中是等价的。但由于初始化值的不同，同一个词最终学习到的两组词向量可能不同。当学习得到所有词向量以后，GloVe模型使用中心词向量与背景词向量之和作为该词的最终词向量。</p>
<p>原理：我们可以构造一个词向量函数使它能有效拟合条件概率比值。我们知道，任意一个这样的比值需要3个词$w_i$、$w_j$和$w_k$。以$w_i$作为中心词的条件概率比值为${p_{ij}}&#x2F;{p_{ik}}$。我们可以找一个函数，它使用词向量来拟合这个条件概率比值<br>$$<br>f(\boldsymbol{u}_j, \boldsymbol{u}<em>k, {\boldsymbol{v}}<em>i) \approx \frac{p</em>{ij}}{p</em>{ik}}.<br>$$</p>
<p>这里函数$f$可能的设计并不唯一，我们只需考虑一种较为合理的可能性。注意到条件概率比值是一个标量，我们可以将$f$限制为一个标量函数：$f(\boldsymbol{u}_j, \boldsymbol{u}_k, {\boldsymbol{v}}_i) &#x3D; f\left((\boldsymbol{u}_j - \boldsymbol{u}_k)^\top {\boldsymbol{v}}_i\right)$。可以考虑一下交换律。</p>
<p>交换索引$j$和$k$后可以看到函数$f$应该满足$f(x)f(-x)&#x3D;1$，因此一种可能是$f(x)&#x3D;\exp(x)$</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220721215435960.png" alt="image-20220721215435960"></p>
<p>本节我们使用torchtext进行练习。下面查看它目前提供的预训练词嵌入的名称。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchtext.vocab <span class="keyword">as</span> vocab</span><br><span class="line"></span><br><span class="line">vocab.pretrained_aliases.keys()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([&#x27;charngram.100d&#x27;, &#x27;fasttext.en.300d&#x27;, &#x27;fasttext.simple.300d&#x27;, &#x27;glove.42B.300d&#x27;, &#x27;glove.840B.300d&#x27;, &#x27;glove.twitter.27B.25d&#x27;, &#x27;glove.twitter.27B.50d&#x27;, &#x27;glove.twitter.27B.100d&#x27;, &#x27;glove.twitter.27B.200d&#x27;, &#x27;glove.6B.50d&#x27;, &#x27;glove.6B.100d&#x27;, &#x27;glove.6B.200d&#x27;, &#x27;glove.6B.300d&#x27;])</span><br></pre></td></tr></table></figure>

<p>下面查看查看该<code>glove</code>词嵌入提供了哪些预训练的模型。每个模型的词向量维度可能不同，或是在不同数据集上预训练得到的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[key <span class="keyword">for</span> key <span class="keyword">in</span> vocab.pretrained_aliases.keys()</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;glove&quot;</span> <span class="keyword">in</span> key]</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;glove.42B.300d&#x27;,</span><br><span class="line"> &#x27;glove.840B.300d&#x27;,</span><br><span class="line"> &#x27;glove.twitter.27B.25d&#x27;,</span><br><span class="line"> &#x27;glove.twitter.27B.50d&#x27;,</span><br><span class="line"> &#x27;glove.twitter.27B.100d&#x27;,</span><br><span class="line"> &#x27;glove.twitter.27B.200d&#x27;,</span><br><span class="line"> &#x27;glove.6B.50d&#x27;,</span><br><span class="line"> &#x27;glove.6B.100d&#x27;,</span><br><span class="line"> &#x27;glove.6B.200d&#x27;,</span><br><span class="line"> &#x27;glove.6B.300d&#x27;]</span><br></pre></td></tr></table></figure>

<p>预训练的GloVe模型的命名规范大致是“模型.（数据集.）数据集词数.词向量维度”。更多信息可以参考GloVe和fastText的项目网站[1,2]。下面我们使用基于维基百科子集预训练的50维GloVe词向量。第一次创建预训练词向量实例时会自动下载相应的词向量到<code>cache</code>指定文件夹（默认为<code>.vector_cache</code>），因此需要联网。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cache_dir = <span class="string">&quot;/Users/tangshusen/Datasets/glove&quot;</span></span><br><span class="line"><span class="comment"># glove = vocab.pretrained_aliases[&quot;glove.6B.50d&quot;](cache=cache_dir)</span></span><br><span class="line">glove = vocab.GloVe(name=<span class="string">&#x27;6B&#x27;</span>, dim=<span class="number">50</span>, cache=cache_dir) <span class="comment"># 与上面等价</span></span><br></pre></td></tr></table></figure>

<p>返回的实例主要有以下三个属性：</p>
<ul>
<li><code>stoi</code>: 词到索引的字典：</li>
<li><code>itos</code>: 一个列表，索引到词的映射；</li>
<li><code>vectors</code>: 词向量。</li>
</ul>
<p>打印词典大小。其中含有40万个词。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;一共包含%d个词。&quot;</span> % <span class="built_in">len</span>(glove.stoi))</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一共包含400000个词。</span><br></pre></td></tr></table></figure>

<p>我们可以通过词来获取它在词典中的索引，也可以通过索引获取词。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">glove.stoi[<span class="string">&#x27;beautiful&#x27;</span>], glove.itos[<span class="number">3366</span>] <span class="comment"># (3366, &#x27;beautiful&#x27;)</span></span><br></pre></td></tr></table></figure>

<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>word2vec和GloVe都将相同的预训练向量分配给同一个词，而&#x3D;&#x3D;不考虑词的上下文（如果有的话）&#x3D;&#x3D;。虑到自然语言中丰富的多义现象和复杂的语义，上下文无关表示具有明显的局限性。这推动了“上下文敏感”词表示的发展，其中词的表征取决于它们的上下文。因此，词元x的上下文敏感表示为$$f(x,c(x))$$,流行的上下文敏感表示包括TagLM（language-model-augmented sequence tagger，语言模型增强的序列标记器） [<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#peters-ammar-bhagavatula-ea-2017">Peters et al., 2017b]</a>、CoVe（Context Vectors，上下文向量） [<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#mccann-bradbury-xiong-ea-2017">McCann et al., 2017]</a>和ELMo（Embeddings from Language Models，来自语言模型的嵌入） [<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#peters-neumann-iyyer-ea-2018">Peters et al., 2018]</a>。</p>
<p>ELMo对上下文进行双向编码，但使用特定于任务的架构；而&#x3D;&#x3D;GPT是任务无关的&#x3D;&#x3D;，但是从左到右编码上下文。将GPT应用于下游任务时，语言模型的输出将被送到一个附加的线性输出层，以预测任务的标签。与ELMo冻结预训练模型的参数不同，GPT在下游任务的监督学习过程中对预训练Transformer解码器中的所有参数进行微调。</p>
<p>BERT（来自Transformers的双向编码器表示）结合了这两个方面的优点。它<strong>对上下文进行双向编码，并且对于大多数的自然语言处理任务 [<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#devlin-chang-lee-ea-2018">Devlin et al., 2018]</a>只需要最少的架构改变</strong>。通过使用预训练的Transformer编码器，BERT能够基于其双向上下文表示任何词元。在下游任务的监督学习过程中，BERT在两个方面与GPT相似。首先，BERT表示将被输入到一个添加的输出层中，根据任务的性质对模型架构进行最小的更改，例如预测每个词元与预测整个序列。其次，对预训练Transformer编码器的所有参数进行微调，而额外的输出层将从头开始训练。</p>
<p>在自然语言处理中，有些任务（如情感分析）以单个文本作为输入，而有些任务（如自然语言推断）以一对文本序列作为输入。BERT输入序列明确地表示单个文本和文本对。当输入为单个文本时，BERT输入序列是特殊类别词元“<cls>”、文本序列的标记、以及特殊分隔词元“<sep>”的连结。当输入为文本对时，BERT输入序列是“<cls>”、第一个文本序列的标记、“<sep>”、第二个文本序列标记、以及“<sep>”的连结。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_tokens_and_segments</span>(<span class="params">tokens_a, tokens_b=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取输入序列的词元及其片段索引&quot;&quot;&quot;</span></span><br><span class="line">    tokens = [<span class="string">&#x27;&lt;cls&gt;&#x27;</span>] + tokens_a + [<span class="string">&#x27;&lt;sep&gt;&#x27;</span>]</span><br><span class="line">    <span class="comment"># 0和1分别标记片段A和B</span></span><br><span class="line">    segments = [<span class="number">0</span>] * (<span class="built_in">len</span>(tokens_a) + <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span> tokens_b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        tokens += tokens_b + [<span class="string">&#x27;&lt;sep&gt;&#x27;</span>]</span><br><span class="line">        segments += [<span class="number">1</span>] * (<span class="built_in">len</span>(tokens_b) + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> tokens, segments</span><br></pre></td></tr></table></figure>

<p>为了区分文本对，根据输入序列学到的片段嵌入eA和eB分别被添加到第一序列和第二序列的词元嵌入中。</p>
<p><img src="C:/Users/mivenis/AppData/Roaming/Typora/typora-user-images/image-20220723115458376.png" alt="image-20220723115458376"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERTEncoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, norm_shape, ffn_num_input,</span></span><br><span class="line"><span class="params">                 ffn_num_hiddens, num_heads, num_layers, dropout,</span></span><br><span class="line"><span class="params">                 max_len=<span class="number">1000</span>, key_size=<span class="number">768</span>, query_size=<span class="number">768</span>, value_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BERTEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.segment_embedding = nn.Embedding(<span class="number">2</span>, num_hiddens)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">f&quot;<span class="subst">&#123;i&#125;</span>&quot;</span>, d2l.EncoderBlock(</span><br><span class="line">                key_size, query_size, value_size, num_hiddens, norm_shape,</span><br><span class="line">                ffn_num_input, ffn_num_hiddens, num_heads, dropout, <span class="literal">True</span>))</span><br><span class="line">        <span class="comment"># 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数</span></span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, max_len,</span><br><span class="line">                                                      num_hiddens))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens, segments, valid_lens</span>):</span><br><span class="line">        <span class="comment"># 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）</span></span><br><span class="line">        X = self.token_embedding(tokens) + self.segment_embedding(segments)</span><br><span class="line">        X = X + self.pos_embedding.data[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blks:</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<h3 id="masked-language-modeling"><a href="#masked-language-modeling" class="headerlink" title="masked language modeling"></a>masked language modeling</h3><p>为了双向编码上下文以表示每个词元，BERT随机掩蔽词元并使用来自双向上下文的词元以自监督的方式预测掩蔽词元。此任务称为<em>掩蔽语言模型</em>。</p>
<p>在这个预训练任务中，将随机选择15%的词元作为预测的掩蔽词元。要预测一个掩蔽词元而不使用标签作弊，一个简单的方法是总是用一个特殊的“<mask>”替换输入序列中的词元。然而，人造特殊词元“<mask>”不会出现在微调中。为了避免预训练和微调之间的这种不匹配，如果为预测而屏蔽词元（例如，在“this movie is great”中选择掩蔽和预测“great”），则在输入中将其替换为：</p>
<ul>
<li>80%时间为特殊的“<mask>“词元（例如，“this movie is great”变为“this movie is<mask>”；</li>
<li>10%时间为随机词元（例如，“this movie is great”变为“this movie is drink”）；</li>
<li>10%时间内为不变的标签词元（例如，“this movie is great”变为“this movie is great”）。</li>
</ul>
<p>请注意，在15%的时间中，有10%的时间插入了随机词元。这种偶然的噪声鼓励BERT在其双向上下文编码中不那么偏向于掩蔽词元（尤其是当标签词元保持不变时）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_replace_mlm_tokens</span>(<span class="params">tokens, candidate_pred_positions, num_mlm_preds,</span></span><br><span class="line"><span class="params">                        vocab</span>):</span><br><span class="line">    <span class="comment"># 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“&lt;mask&gt;”或随机词元</span></span><br><span class="line">    mlm_input_tokens = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">    pred_positions_and_labels = []</span><br><span class="line">    <span class="comment"># 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测</span></span><br><span class="line">    random.shuffle(candidate_pred_positions)</span><br><span class="line">    <span class="keyword">for</span> mlm_pred_position <span class="keyword">in</span> candidate_pred_positions:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(pred_positions_and_labels) &gt;= num_mlm_preds:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        masked_token = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 80%的时间：将词替换为“&lt;mask&gt;”词元</span></span><br><span class="line">        <span class="keyword">if</span> random.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">            masked_token = <span class="string">&#x27;&lt;mask&gt;&#x27;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 10%的时间：保持词不变</span></span><br><span class="line">            <span class="keyword">if</span> random.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">                masked_token = tokens[mlm_pred_position]</span><br><span class="line">            <span class="comment"># 10%的时间：用随机词替换该词</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                masked_token = random.choice(vocab.idx_to_token)</span><br><span class="line">        mlm_input_tokens[mlm_pred_position] = masked_token</span><br><span class="line">        pred_positions_and_labels.append(</span><br><span class="line">            (mlm_pred_position, tokens[mlm_pred_position]))</span><br><span class="line">    <span class="keyword">return</span> mlm_input_tokens, pred_positions_and_labels</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MaskLM</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT的掩蔽语言模型任务&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, num_inputs=<span class="number">768</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MaskLM, self).__init__(**kwargs)</span><br><span class="line">        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.LayerNorm(num_hiddens),</span><br><span class="line">                                 nn.Linear(num_hiddens, vocab_size))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, pred_positions</span>):</span><br><span class="line">        num_pred_positions = pred_positions.shape[<span class="number">1</span>]</span><br><span class="line">        pred_positions = pred_positions.reshape(-<span class="number">1</span>)</span><br><span class="line">        batch_size = X.shape[<span class="number">0</span>]</span><br><span class="line">        batch_idx = torch.arange(<span class="number">0</span>, batch_size)</span><br><span class="line">        <span class="comment"># 假设batch_size=2，num_pred_positions=3</span></span><br><span class="line">        <span class="comment"># 那么batch_idx是np.array（[0,0,0,1,1,1]）</span></span><br><span class="line">        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)</span><br><span class="line">        masked_X = X[batch_idx, pred_positions]<span class="comment">##最后一维是768</span></span><br><span class="line">        masked_X = masked_X.reshape((batch_size, num_pred_positions, -<span class="number">1</span>))</span><br><span class="line">        mlm_Y_hat = self.mlp(masked_X)</span><br><span class="line">        <span class="keyword">return</span> mlm_Y_hat</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mlm_Y = torch.tensor([[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]])<span class="comment">##这个为掩码下的预测次元的真实标签</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">mlm_l = loss(mlm_Y_hat.reshape((-<span class="number">1</span>, vocab_size)), mlm_Y.reshape(-<span class="number">1</span>))</span><br><span class="line">mlm_l.shape</span><br></pre></td></tr></table></figure>

<h3 id="next-sentence-prediction"><a href="#next-sentence-prediction" class="headerlink" title="next sentence prediction"></a>next sentence prediction</h3><p>尽管掩蔽语言建模能够编码双向上下文来表示单词，但它不能显式地建模文本对之间的逻辑关系。为了帮助理解两个文本序列之间的关系，BERT在预训练中考虑了一个二元分类任务——<em>下一句预测</em>。在为预训练生成句子对时，有一半的时间它们确实是标签为“真”的连续句子；在另一半的时间里，第二个句子是从语料库中随机抽取的，标记为“假”。</p>
<p>下面的<code>NextSentencePred</code>类使用单隐藏层的多层感知机来预测第二个句子是否是BERT输入序列中第一个句子的下一个句子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NextSentencePred</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT的下一句预测任务&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(NextSentencePred, self).__init__(**kwargs)</span><br><span class="line">        self.output = nn.Linear(num_inputs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># X的形状：(batchsize,num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> self.output(X)</span><br></pre></td></tr></table></figure>

<ul>
<li>预训练包括两个任务：掩蔽语言模型和下一句预测。前者能够编码双向上下文来表示单词，而后者则显式地建模文本对之间的逻辑关系。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERTModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, norm_shape, ffn_num_input,</span></span><br><span class="line"><span class="params">                 ffn_num_hiddens, num_heads, num_layers, dropout,</span></span><br><span class="line"><span class="params">                 max_len=<span class="number">1000</span>, key_size=<span class="number">768</span>, query_size=<span class="number">768</span>, value_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 hid_in_features=<span class="number">768</span>, mlm_in_features=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 nsp_in_features=<span class="number">768</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BERTModel, self).__init__()</span><br><span class="line">        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,</span><br><span class="line">                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,</span><br><span class="line">                    dropout, max_len=max_len, key_size=key_size,</span><br><span class="line">                    query_size=query_size, value_size=value_size)</span><br><span class="line">        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),</span><br><span class="line">                                    nn.Tanh())</span><br><span class="line">        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)</span><br><span class="line">        self.nsp = NextSentencePred(nsp_in_features)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens, segments, valid_lens=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                pred_positions=<span class="literal">None</span></span>):</span><br><span class="line">        encoded_X = self.encoder(tokens, segments, valid_lens)</span><br><span class="line">        <span class="keyword">if</span> pred_positions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mlm_Y_hat = self.mlm(encoded_X, pred_positions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mlm_Y_hat = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 用于下一句预测的多层感知机分类器的隐藏层，0是“&lt;cls&gt;”标记的索引</span></span><br><span class="line">        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, <span class="number">0</span>, :]))</span><br><span class="line">        <span class="keyword">return</span> encoded_X, mlm_Y_hat, nsp_Y_hat</span><br></pre></td></tr></table></figure>

<p>由于Transformer编码器中的自注意力，&#x3D;&#x3D;特殊词元“<cls>”的BERT表示已经对输入的两个句子进行了编码&#x3D;&#x3D;。因此，多层感知机分类器的输出层（<code>self.output</code>）以<code>X</code>作为输入，其中<code>X</code>是多层感知机隐藏层的输出，而MLP隐藏层的输入是编码后的“<cls>”词元。因为与文本中已有的其它词相比，&#x3D;&#x3D;这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义&#x3D;&#x3D;。</p>
<p>具体来说，self-attention是用文本中的其它词来增强目标词的语义表示，但是&#x3D;&#x3D;目标词本身的语义还是会占主要部分&#x3D;&#x3D;的，因此，经过BERT的12层，每次词的embedding融合了所有词的信息，可以去更好的表示自己的语义。</p>
<p>而[CLS]位本身没有语义，经过12层，得到的是attention后所有词的加权平均，相比其他正常词，可以更好的表征句子语义。</p>
<p>当然，也可以通过对最后一层所有词的embedding做pooling去表征句子语义。</p>
<h3 id="train-bert"><a href="#train-bert" class="headerlink" title="train bert"></a>train bert</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_bert</span>(<span class="params">train_iter, net, loss, vocab_size, devices, num_steps</span>):</span><br><span class="line">    net = nn.DataParallel(net, device_ids=devices).to(devices[<span class="number">0</span>])</span><br><span class="line">    trainer = torch.optim.Adam(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    step, timer = <span class="number">0</span>, d2l.Timer()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;step&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">1</span>, num_steps], legend=[<span class="string">&#x27;mlm&#x27;</span>, <span class="string">&#x27;nsp&#x27;</span>])</span><br><span class="line">    <span class="comment"># 遮蔽语言模型损失的和，下一句预测任务损失的和，句子对的数量，计数</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">4</span>)</span><br><span class="line">    num_steps_reached = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> step &lt; num_steps <span class="keyword">and</span> <span class="keyword">not</span> num_steps_reached:</span><br><span class="line">        <span class="keyword">for</span> tokens_X, segments_X, valid_lens_x, pred_positions_X,\</span><br><span class="line">            mlm_weights_X, mlm_Y, nsp_y <span class="keyword">in</span> train_iter:</span><br><span class="line">            tokens_X = tokens_X.to(devices[<span class="number">0</span>])</span><br><span class="line">            segments_X = segments_X.to(devices[<span class="number">0</span>])</span><br><span class="line">            valid_lens_x = valid_lens_x.to(devices[<span class="number">0</span>])</span><br><span class="line">            pred_positions_X = pred_positions_X.to(devices[<span class="number">0</span>])</span><br><span class="line">            mlm_weights_X = mlm_weights_X.to(devices[<span class="number">0</span>])</span><br><span class="line">            mlm_Y, nsp_y = mlm_Y.to(devices[<span class="number">0</span>]), nsp_y.to(devices[<span class="number">0</span>])</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            timer.start()</span><br><span class="line">            mlm_l, nsp_l, l = _get_batch_loss_bert(</span><br><span class="line">                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,</span><br><span class="line">                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">            metric.add(mlm_l, nsp_l, tokens_X.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">            timer.stop()</span><br><span class="line">            animator.add(step + <span class="number">1</span>,</span><br><span class="line">                         (metric[<span class="number">0</span>] / metric[<span class="number">3</span>], metric[<span class="number">1</span>] / metric[<span class="number">3</span>]))</span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> step == num_steps:</span><br><span class="line">                num_steps_reached = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;MLM loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">3</span>]:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;NSP loss <span class="subst">&#123;metric[<span class="number">1</span>] / metric[<span class="number">3</span>]:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> sentence pairs/sec on &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">str</span>(devices)&#125;</span>&#x27;</span>)</span><br><span class="line">          </span><br><span class="line">          </span><br></pre></td></tr></table></figure>



<h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p><img src="https://img-blog.csdnimg.cn/294c69bf8c69429b904e85b288dbc86c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTU3Nzg2NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45577864/article/details/119651372">https://blog.csdn.net/weixin_45577864/article/details/119651372</a></p>
<h2 id="Sub-word-Tokenization"><a href="#Sub-word-Tokenization" class="headerlink" title="Sub word Tokenization"></a>Sub word Tokenization</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86965595">https://zhuanlan.zhihu.com/p/86965595</a></p>
<h3 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h3><p>BPE(字节对)编码或二元编码是一种简单的数据压缩形式，其中最常见的<strong>一对连续字节数据被替换为该数据中不存在的字节</strong>[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86965595#ref_2">2]</a>。 后期使用时需要一个替换表来重建原始数据。OpenAI <a href="https://link.zhihu.com/?target=https://towardsdatascience.com/too-powerful-nlp-model-generative-pre-training-2-4cc6afb6655">GPT-2 </a>与Facebook <a href="https://link.zhihu.com/?target=https://github.com/pytorch/fairseq/tree/master/examples/roberta">RoBERTa</a>均采用此方法构建subword vector.</p>
<ul>
<li><p>优点</p>
</li>
<li><ul>
<li>可以有效地<strong>平衡词汇表大小</strong>和步数(编码句子所需的token数量)。</li>
</ul>
</li>
<li><p>缺点</p>
</li>
<li><ul>
<li>基于贪婪和确定的符号替换，不能提供带概率的多个分片结果。</li>
</ul>
</li>
</ul>
<ol>
<li>准备足够大的训练语料</li>
<li>确定期望的subword词表大小</li>
<li>将<strong>单词拆分为字符序列（subword)并在末尾添加后缀“ &lt;&#x2F; w&gt;”</strong>，统计单词频率。 本阶段的subword的<strong>粒度是字符</strong>。 例如，“ low”的频率为5，那么我们将其改写为“ l o w &lt;&#x2F; w&gt;”：5</li>
<li><strong>统计每一个连续字节对的出现频率，选择最高频者合并成新的subword</strong></li>
<li>重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1</li>
</ol>
<p>停止符”</w>“的意义在于表示subword是词后缀。举例来说：”st”字词不加”</w>“可以出现在词首如”st ar”，加了”</w>“表明改字词位于词尾，如”wide st</w>“，二者意义截然不同。</p>
<p>每次合并后词表可能出现3种变化：</p>
<ul>
<li>+1，表明加入合并后的新字词，同时原来的2个子词还保留（2个字词不是完全同时连续出现）</li>
<li>+0，表明加入合并后的新字词，同时原来的2个子词中一个保留，一个被消解（一个字词完全随着另一个字词的出现而紧跟着出现）</li>
<li>-1，表明加入合并后的新字词，同时原来的2个子词都被消解（2个字词同时连续出现）</li>
</ul>
<p>实际上，随着合并的次数增加，词表大小通常先增加后减小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re, collections</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_stats</span>(<span class="params">vocab</span>):</span><br><span class="line">    pairs = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(symbols)-<span class="number">1</span>):</span><br><span class="line">            pairs[symbols[i],symbols[i+<span class="number">1</span>]] += freq</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">merge_vocab</span>(<span class="params">pair, v_in</span>):</span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(<span class="string">&#x27; &#x27;</span>.join(pair))</span><br><span class="line">    p = re.<span class="built_in">compile</span>(<span class="string">r&#x27;(?&lt;!\S)&#x27;</span> + bigram + <span class="string">r&#x27;(?!\S)&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        w_out = p.sub(<span class="string">&#x27;&#x27;</span>.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    <span class="keyword">return</span> v_out</span><br><span class="line"></span><br><span class="line">vocab = &#123;<span class="string">&#x27;l o w &lt;/w&gt;&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;l o w e r &lt;/w&gt;&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;n e w e s t &lt;/w&gt;&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;w i d e s t &lt;/w&gt;&#x27;</span>: <span class="number">3</span>&#125;</span><br><span class="line">num_merges = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_merges):</span><br><span class="line">    pairs = get_stats(vocab)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pairs:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    best = <span class="built_in">max</span>(pairs, key=pairs.get)</span><br><span class="line">    vocab = merge_vocab(best, vocab)</span><br><span class="line">    <span class="built_in">print</span>(best)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print output</span></span><br><span class="line"><span class="comment"># (&#x27;e&#x27;, &#x27;s&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;es&#x27;, &#x27;t&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;est&#x27;, &#x27;&lt;/w&gt;&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;l&#x27;, &#x27;o&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;lo&#x27;, &#x27;w&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;n&#x27;, &#x27;e&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;ne&#x27;, &#x27;w&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;new&#x27;, &#x27;est&lt;/w&gt;&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;low&#x27;, &#x27;&lt;/w&gt;&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;w&#x27;, &#x27;i&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;wi&#x27;, &#x27;d&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;wid&#x27;, &#x27;est&lt;/w&gt;&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;low&#x27;, &#x27;e&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;lowe&#x27;, &#x27;r&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;lower&#x27;, &#x27;&lt;/w&gt;&#x27;)</span></span><br></pre></td></tr></table></figure>

<ul>
<li>编码</li>
</ul>
<p>在之前的算法中，我们已经得到了subword的词表，<strong>对该词表按照子词长度由大到小排序</strong>。编码时，对于每个单词，遍历排好序的子词词表寻找是否有token是当前单词的子字符串，如果有，则该token是表示单词的tokens之一。</p>
<p>我们<strong>从最长的token迭代到最短的token</strong>，尝试将每个单词中的子字符串替换为token。 最终，我们将迭代所有tokens，并将所有子字符串替换为tokens。 如果仍然有子字符串没被替换但所有token都已迭代完毕，则将剩余的子词替换为特殊token，如<unk>。</p>
<p>例子</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 给定单词序列</span><br><span class="line">[“the&lt;/w&gt;”, “highest&lt;/w&gt;”, “mountain&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"># 假设已有排好序的subword词表</span><br><span class="line">[“errrr&lt;/w&gt;”, “tain&lt;/w&gt;”, “moun”, “est&lt;/w&gt;”, “high”, “the&lt;/w&gt;”, “a&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"># 迭代结果</span><br><span class="line">&quot;the&lt;/w&gt;&quot; -&gt; [&quot;the&lt;/w&gt;&quot;]</span><br><span class="line">&quot;highest&lt;/w&gt;&quot; -&gt; [&quot;high&quot;, &quot;est&lt;/w&gt;&quot;]</span><br><span class="line">&quot;mountain&lt;/w&gt;&quot; -&gt; [&quot;moun&quot;, &quot;tain&lt;/w&gt;&quot;]</span><br></pre></td></tr></table></figure>

<p>编码的计算量很大。 在实践中，我们可以pre-tokenize所有单词，并在词典中保存单词tokenize的结果。 如果我们看到字典中不存在的未知单词。 我们应用上述编码方法对单词进行tokenize，然后将新单词的tokenization添加到字典中备用。</p>
<ul>
<li>解码</li>
</ul>
<p>将所有的tokens拼在一起。</p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编码序列</span></span><br><span class="line">[“the&lt;/w&gt;”, “high”, “est&lt;/w&gt;”, “moun”, “tain&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解码序列</span></span><br><span class="line">“the&lt;/w&gt; highest&lt;/w&gt; mountain&lt;/w&gt;”</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/07/11/csapp_note/" rel="prev" title="">
      <i class="fa fa-chevron-left"></i> 
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/07/11/Docker/" rel="next" title="">
       <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#DEEP-LEARNING"><span class="nav-number">1.</span> <span class="nav-text">DEEP LEARNING</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#example"><span class="nav-number">1.1.</span> <span class="nav-text">example</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#data"><span class="nav-number">1.2.</span> <span class="nav-text">data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#os-x2F-open"><span class="nav-number">1.3.</span> <span class="nav-text">os&#x2F;open</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#function"><span class="nav-number">1.4.</span> <span class="nav-text">function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#grad"><span class="nav-number">1.5.</span> <span class="nav-text">grad</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Probability"><span class="nav-number">1.6.</span> <span class="nav-text">Probability</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#init"><span class="nav-number">1.7.</span> <span class="nav-text">init</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#loss"><span class="nav-number">1.8.</span> <span class="nav-text">loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#network"><span class="nav-number">1.9.</span> <span class="nav-text">network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B4%E6%8E%A5%E7%BB%A7%E6%89%BFmodule"><span class="nav-number">1.9.1.</span> <span class="nav-text">直接继承module</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%A7%E6%89%BFmodule%E7%9A%84%E5%AD%90%E7%B1%BB"><span class="nav-number">1.9.2.</span> <span class="nav-text">继承module的子类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#sequential"><span class="nav-number">1.9.2.1.</span> <span class="nav-text">sequential</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#module-list"><span class="nav-number">1.9.2.2.</span> <span class="nav-text">module list</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#module-dict"><span class="nav-number">1.9.2.3.</span> <span class="nav-text">module dict</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#parameters"><span class="nav-number">1.9.2.4.</span> <span class="nav-text">parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#save-and-load"><span class="nav-number">1.9.2.5.</span> <span class="nav-text">save and load</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fine-tuning"><span class="nav-number">1.10.</span> <span class="nav-text">fine tuning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BN-amp-LN"><span class="nav-number">1.11.</span> <span class="nav-text">BN&amp;LN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BN"><span class="nav-number">1.11.1.</span> <span class="nav-text">BN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LN"><span class="nav-number">1.11.2.</span> <span class="nav-text">LN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%82%E5%90%8C"><span class="nav-number">1.11.3.</span> <span class="nav-text">异同</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ERROR"><span class="nav-number">1.12.</span> <span class="nav-text">ERROR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Propagation"><span class="nav-number">1.13.</span> <span class="nav-text">Propagation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPU"><span class="nav-number">1.14.</span> <span class="nav-text">GPU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optim"><span class="nav-number">1.15.</span> <span class="nav-text">Optim</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CrossEntropyLoss"><span class="nav-number">1.15.1.</span> <span class="nav-text">CrossEntropyLoss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#drop-out"><span class="nav-number">1.15.2.</span> <span class="nav-text">drop_out</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#wegiht-decay"><span class="nav-number">1.15.3.</span> <span class="nav-text">wegiht decay</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam"><span class="nav-number">1.15.4.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#scheduler"><span class="nav-number">1.15.5.</span> <span class="nav-text">scheduler</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MLP"><span class="nav-number">1.16.</span> <span class="nav-text">MLP</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#activate-function"><span class="nav-number">1.16.1.</span> <span class="nav-text">activate function</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN"><span class="nav-number">1.17.</span> <span class="nav-text">CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#conv"><span class="nav-number">1.17.1.</span> <span class="nav-text">conv</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pooling"><span class="nav-number">1.17.2.</span> <span class="nav-text">pooling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CrossEntropyLoss-1"><span class="nav-number">1.17.3.</span> <span class="nav-text">CrossEntropyLoss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet"><span class="nav-number">1.17.4.</span> <span class="nav-text">ResNet</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN"><span class="nav-number">1.18.</span> <span class="nav-text">RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Base"><span class="nav-number">1.18.1.</span> <span class="nav-text">Base</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BPTT"><span class="nav-number">1.18.2.</span> <span class="nav-text">BPTT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GRU"><span class="nav-number">1.18.3.</span> <span class="nav-text">GRU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM"><span class="nav-number">1.18.4.</span> <span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bidirectional"><span class="nav-number">1.18.5.</span> <span class="nav-text">bidirectional</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Seq2Seq"><span class="nav-number">1.19.</span> <span class="nav-text">Seq2Seq</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#encoder"><span class="nav-number">1.19.1.</span> <span class="nav-text">encoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decoder"><span class="nav-number">1.19.2.</span> <span class="nav-text">decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-1"><span class="nav-number">1.19.3.</span> <span class="nav-text">loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#evaluate"><span class="nav-number">1.19.4.</span> <span class="nav-text">evaluate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#beam-search"><span class="nav-number">1.19.5.</span> <span class="nav-text">beam search</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#EncoderDecoder-Strcture"><span class="nav-number">1.19.6.</span> <span class="nav-text">EncoderDecoder Strcture</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention-is-all-u-need"><span class="nav-number">1.20.</span> <span class="nav-text">Attention is all u need</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention"><span class="nav-number">1.20.1.</span> <span class="nav-text">Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multihead"><span class="nav-number">1.20.2.</span> <span class="nav-text">Multihead</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attention-and-position-encoding"><span class="nav-number">1.20.3.</span> <span class="nav-text">self-attention and position-encoding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer"><span class="nav-number">1.21.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#encoder-1"><span class="nav-number">1.21.1.</span> <span class="nav-text">encoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decoder-1"><span class="nav-number">1.21.2.</span> <span class="nav-text">decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#code"><span class="nav-number">1.21.3.</span> <span class="nav-text">code</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#word2vec"><span class="nav-number">1.22.</span> <span class="nav-text">word2vec</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#skip-gram"><span class="nav-number">1.22.1.</span> <span class="nav-text">skip-gram</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CBOW"><span class="nav-number">1.22.2.</span> <span class="nav-text">CBOW</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#approximate-training"><span class="nav-number">1.22.3.</span> <span class="nav-text">approximate training</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Subword-embedding"><span class="nav-number">1.23.</span> <span class="nav-text">Subword embedding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GloVe"><span class="nav-number">1.24.</span> <span class="nav-text">GloVe</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BERT"><span class="nav-number">1.25.</span> <span class="nav-text">BERT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#masked-language-modeling"><span class="nav-number">1.25.1.</span> <span class="nav-text">masked language modeling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#next-sentence-prediction"><span class="nav-number">1.25.2.</span> <span class="nav-text">next sentence prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#train-bert"><span class="nav-number">1.25.3.</span> <span class="nav-text">train bert</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPT"><span class="nav-number">1.26.</span> <span class="nav-text">GPT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sub-word-Tokenization"><span class="nav-number">1.27.</span> <span class="nav-text">Sub word Tokenization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BPE"><span class="nav-number">1.27.1.</span> <span class="nav-text">BPE</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Geng</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Geng</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":145,"height":315},"mobile":{"show":true,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.8},"log":false});</script></body>
</html>
